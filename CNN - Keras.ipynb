{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "np.random.seed(666)\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, x, y):\n",
    "    res = model.evaluate(x,y)\n",
    "    print('loss:',res[0], 'acc:', res[1])\n",
    "    \n",
    "def write_subs(x, ids, filename = 'subs'):\n",
    "    test_predictions = model.predict(x)\n",
    "    test_predictions = test_predictions.reshape((test_predictions.shape[0]))\n",
    "    test_predictions = ['{:f}'.format(item) for item in test_predictions]\n",
    "\n",
    "    submission = pd.DataFrame({'id': ids, 'is_iceberg': test_predictions})\n",
    "    submission.to_csv(filename, index = False)\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not reserve memory block",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-a49227939176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minc_angle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minc_angle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'na'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minc_angle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minc_angle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ramdoot/anaconda2/lib/python2.7/site-packages/pandas/io/json/json.pyc\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines)\u001b[0m\n\u001b[1;32m    352\u001b[0m         obj = FrameParser(json, orient, dtype, convert_axes, convert_dates,\n\u001b[1;32m    353\u001b[0m                           \u001b[0mkeep_default_dates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m                           date_unit).parse()\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'series'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ramdoot/anaconda2/lib/python2.7/site-packages/pandas/io/json/json.pyc\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ramdoot/anaconda2/lib/python2.7/site-packages/pandas/io/json/json.pyc\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m--> 639\u001b[0;31m                 loads(json, precise_float=self.precise_float), dtype=None)\n\u001b[0m\u001b[1;32m    640\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             decoded = dict((str(k), v)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not reserve memory block"
     ]
    }
   ],
   "source": [
    "#Load data\n",
    "train = pd.read_json(\"train.json\")\n",
    "test = pd.read_json(\"test.json\")\n",
    "train.inc_angle = train.inc_angle.replace('na', 0)\n",
    "train.inc_angle = train.inc_angle.astype(float).fillna(0.0)\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "x_band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\n",
    "x_band2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_2\"]])\n",
    "X_train = np.concatenate([x_band1[:, :, :, np.newaxis]\n",
    "                          , x_band2[:, :, :, np.newaxis]], axis=-1)\n",
    "X_angle_train = np.array(train.inc_angle)\n",
    "y_train = np.array(train[\"is_iceberg\"])\n",
    "\n",
    "# Test data\n",
    "x_band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_1\"]])\n",
    "x_band2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_2\"]])\n",
    "X_test = np.concatenate([x_band1[:, :, :, np.newaxis]\n",
    "                          , x_band2[:, :, :, np.newaxis]], axis=-1)\n",
    "X_angle_test = np.array(test.inc_angle)\n",
    "\n",
    "\n",
    "X_train, X_valid, X_angle_train, X_angle_valid, y_train, y_valid = train_test_split(X_train\n",
    "                    , X_angle_train, y_train, random_state=123, train_size=0.6)\n",
    "X_valid, X_testing, y_valid, y_testing = train_test_split(X_valid, y_valid, random_state=123, train_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPool2D\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acti = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(BatchNormalization(input_shape=(75, 75, 3)))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3,3), activation=acti))\n",
    "model.add(Conv2D(32, kernel_size=(3,3), activation=acti))\n",
    "model.add(Conv2D(32, kernel_size=(3,3), activation=acti))\n",
    "model.add(Conv2D(32, kernel_size=(3,3), activation=acti))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(48, kernel_size=(3,3), activation=acti))\n",
    "model.add(Conv2D(48, kernel_size=(3,3), activation=acti))\n",
    "model.add(Conv2D(48, kernel_size=(3,3), activation=acti))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation=acti))\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation=acti))\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation=acti))\n",
    "model.add(GlobalMaxPool2D())\n",
    "#model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model.add(Dense(300, activation=acti))\n",
    "model.add(Dropout(0.4)) \n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model.add(Dense(128, activation=acti))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "\n",
    "def get_callbacks(filepath, patience=10):\n",
    "    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "    \n",
    "    return [es, msave]\n",
    "\n",
    "callbacks = get_callbacks('./model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_gen = ImageDataGenerator(\n",
    "        rotation_range=70,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        rescale=1./255,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "\n",
    "test_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "training_set = train_gen.flow(X_train, y_train, batch_size=32)\n",
    "valid_set = test_gen.flow(X_valid, y_valid, batch_size=32)\n",
    "test_set = test_gen.flow(X_test, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      " 327/3000 [==>...........................] - ETA: 252s - loss: 0.6811 - mean_absolute_error: 0.4050- E"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-77a1e45b7daf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m model.fit_generator(training_set, steps_per_epoch=3000, epochs=1000, \n\u001b[1;32m      3\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     validation_steps=400, workers=7)\n\u001b[0m",
      "\u001b[0;32m/home/ramdoot/anaconda2/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ramdoot/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\u001b[0m\n\u001b[1;32m   1119\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ramdoot/anaconda2/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ramdoot/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2040\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2041\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2042\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2044\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ramdoot/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1760\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1762\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ramdoot/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ramdoot/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ramdoot/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ramdoot/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ramdoot/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ramdoot/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.adam(lr=0.001, decay=0.25), metrics=['mae'])\n",
    "model.fit_generator(training_set, steps_per_epoch=3000, epochs=1000, \n",
    "                    callbacks=callbacks, validation_data=valid_set, \n",
    "                    validation_steps=400, workers=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25241673039867457, 0.16393957372945869]\n",
      "[0.27912027280087598, 0.17202319949321723]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate_generator(training_set, 4000))\n",
    "print(model.evaluate_generator(valid_set, 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>is_iceberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5941774d</td>\n",
       "      <td>0.026564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4023181e</td>\n",
       "      <td>0.371797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b20200e4</td>\n",
       "      <td>0.000140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e7f018bb</td>\n",
       "      <td>0.877413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4371c8c3</td>\n",
       "      <td>0.042351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a8d9b1fd</td>\n",
       "      <td>0.020246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29e7727e</td>\n",
       "      <td>0.079988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>92a51ffb</td>\n",
       "      <td>0.969216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>c769ac97</td>\n",
       "      <td>0.010322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>aee0547d</td>\n",
       "      <td>0.000406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id is_iceberg\n",
       "0  5941774d   0.026564\n",
       "1  4023181e   0.371797\n",
       "2  b20200e4   0.000140\n",
       "3  e7f018bb   0.877413\n",
       "4  4371c8c3   0.042351\n",
       "5  a8d9b1fd   0.020246\n",
       "6  29e7727e   0.079988\n",
       "7  92a51ffb   0.969216\n",
       "8  c769ac97   0.010322\n",
       "9  aee0547d   0.000406"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions = model.predict_generator(test_set, steps = test.shape[0], use_multiprocessing=True)\n",
    "test_predictions = test_predictions.reshape((test_predictions.shape[0]))\n",
    "test_predictions = ['{:f}'.format(item) for item in test_predictions]\n",
    "\n",
    "submission = pd.DataFrame({'id': test[\"id\"], 'is_iceberg': test_predictions})\n",
    "submission.to_csv('sub172.csv', index = False)\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8424, 2)"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New model with + featuremaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_411 (Bat (None, 75, 75, 3)         12        \n",
      "_________________________________________________________________\n",
      "conv2d_436 (Conv2D)          (None, 73, 73, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_79 (MaxPooling (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_412 (Bat (None, 36, 36, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_185 (Dropout)        (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_437 (Conv2D)          (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_80 (MaxPooling (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_413 (Bat (None, 17, 17, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_186 (Dropout)        (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_438 (Conv2D)          (None, 15, 15, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_81 (MaxPooling (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_414 (Bat (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_187 (Dropout)        (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_439 (Conv2D)          (None, 5, 5, 256)         295168    \n",
      "_________________________________________________________________\n",
      "conv2d_440 (Conv2D)          (None, 3, 3, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_2 (Glob (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_415 (Bat (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_188 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_189 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_416 (Bat (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_190 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_417 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,968,845\n",
      "Trainable params: 1,965,831\n",
      "Non-trainable params: 3,014\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "acti = 'relu'\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(BatchNormalization(input_shape=(75, 75, 3)))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3,3), activation=acti))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation=acti))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation=acti))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3,3), activation=acti))\n",
    "model.add(Conv2D(512, kernel_size=(3,3), activation=acti))\n",
    "model.add(GlobalMaxPool2D())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "model.add(Dense(512, activation=acti))\n",
    "model.add(Dropout(0.6)) \n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model.add(Dense(256, activation=acti))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        zoom_range = 0.1,\n",
    "        width_shift_range=0.3,\n",
    "        height_shift_range=0.3,\n",
    "        rescale=1./255,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "\n",
    "test_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "training_set = train_gen.flow(X_train, y_train, batch_size=264)\n",
    "valid_set = test_gen.flow(X_valid, y_valid, batch_size=264)\n",
    "test_set = test_gen.flow(X_test, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "31/31 [==============================] - 12s - loss: 0.7451 - mean_absolute_error: 0.4191 - val_loss: 0.6954 - val_mean_absolute_error: 0.4977\n",
      "Epoch 2/80\n",
      "31/31 [==============================] - 8s - loss: 0.6680 - mean_absolute_error: 0.3829 - val_loss: 0.7186 - val_mean_absolute_error: 0.5034\n",
      "Epoch 3/80\n",
      "31/31 [==============================] - 8s - loss: 0.5808 - mean_absolute_error: 0.3591 - val_loss: 0.7637 - val_mean_absolute_error: 0.5059\n",
      "Epoch 4/80\n",
      "31/31 [==============================] - 8s - loss: 0.5733 - mean_absolute_error: 0.3467 - val_loss: 1.2175 - val_mean_absolute_error: 0.5141\n",
      "Epoch 5/80\n",
      "31/31 [==============================] - 8s - loss: 0.5132 - mean_absolute_error: 0.3237 - val_loss: 1.0950 - val_mean_absolute_error: 0.5118\n",
      "Epoch 6/80\n",
      " 9/31 [=======>......................] - ETA: 1s - loss: 0.5527 - mean_absolute_error: 0.3179"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-79315a9de324>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mae'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m model.fit_generator(training_set, steps_per_epoch=steps_per_epoch, epochs=epochs, \n\u001b[0;32m----> 7\u001b[0;31m                     callbacks=callbacks, validation_data=valid_set, validation_steps=400)\n\u001b[0m",
      "\u001b[0;32m/home/ramdoot/anaconda2/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ramdoot/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\u001b[0m\n\u001b[1;32m   1119\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ramdoot/anaconda2/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ramdoot/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2040\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2041\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2042\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2044\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ramdoot/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1760\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1762\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ramdoot/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ramdoot/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ramdoot/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ramdoot/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ramdoot/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ramdoot/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 264\n",
    "epochs = 80\n",
    "steps_per_epoch=2**13/batch_size\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.adam(lr=0.001, decay=0.005), metrics=['mae'])\n",
    "model.fit_generator(training_set, steps_per_epoch=steps_per_epoch, epochs=epochs, \n",
    "                    callbacks=callbacks, validation_data=valid_set, validation_steps=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(model.evaluate_generator(training_set, steps_per_epoch))\n",
    "print(model.evaluate_generator(valid_set, 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>is_iceberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5941774d</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4023181e</td>\n",
       "      <td>0.998655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b20200e4</td>\n",
       "      <td>0.002371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e7f018bb</td>\n",
       "      <td>0.999963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4371c8c3</td>\n",
       "      <td>0.634913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a8d9b1fd</td>\n",
       "      <td>0.909739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29e7727e</td>\n",
       "      <td>0.000096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>92a51ffb</td>\n",
       "      <td>0.999552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>c769ac97</td>\n",
       "      <td>0.019017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>aee0547d</td>\n",
       "      <td>0.000684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id is_iceberg\n",
       "0  5941774d   0.000025\n",
       "1  4023181e   0.998655\n",
       "2  b20200e4   0.002371\n",
       "3  e7f018bb   0.999963\n",
       "4  4371c8c3   0.634913\n",
       "5  a8d9b1fd   0.909739\n",
       "6  29e7727e   0.000096\n",
       "7  92a51ffb   0.999552\n",
       "8  c769ac97   0.019017\n",
       "9  aee0547d   0.000684"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions = model.predict_generator(test_set, steps = test.shape[0])\n",
    "test_predictions = test_predictions.reshape((test_predictions.shape[0]))\n",
    "test_predictions = ['{:f}'.format(item) for item in test_predictions]\n",
    "\n",
    "submission = pd.DataFrame({'id': test[\"id\"], 'is_iceberg': test_predictions})\n",
    "submission.to_csv('./submitions/sub098_32-64-128-256-512-1024.csv', index = False)\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_31 (InputLayer)            (None, 75, 75, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_497 (Conv2D)              (None, 35, 35, 32)    4736        input_31[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_474 (BatchNo (None, 35, 35, 32)    128         conv2d_497[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_416 (LeakyReLU)      (None, 35, 35, 32)    0           batch_normalization_474[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_88 (MaxPooling2D)  (None, 17, 17, 32)    0           leaky_re_lu_416[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_498 (Conv2D)              (None, 17, 17, 16)    528         max_pooling2d_88[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_475 (BatchNo (None, 17, 17, 16)    64          conv2d_498[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_417 (LeakyReLU)      (None, 17, 17, 16)    0           batch_normalization_475[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_499 (Conv2D)              (None, 17, 17, 16)    2320        leaky_re_lu_417[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_476 (BatchNo (None, 17, 17, 16)    64          conv2d_499[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_418 (LeakyReLU)      (None, 17, 17, 16)    0           batch_normalization_476[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_501 (Conv2D)              (None, 17, 17, 32)    1056        max_pooling2d_88[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_500 (Conv2D)              (None, 17, 17, 32)    544         leaky_re_lu_418[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_478 (BatchNo (None, 17, 17, 32)    128         conv2d_501[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_477 (BatchNo (None, 17, 17, 32)    128         conv2d_500[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "add_117 (Add)                    (None, 17, 17, 32)    0           batch_normalization_478[0][0]    \n",
      "                                                                   batch_normalization_477[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_419 (LeakyReLU)      (None, 17, 17, 32)    0           add_117[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_502 (Conv2D)              (None, 17, 17, 16)    528         leaky_re_lu_419[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_479 (BatchNo (None, 17, 17, 16)    64          conv2d_502[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_420 (LeakyReLU)      (None, 17, 17, 16)    0           batch_normalization_479[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_503 (Conv2D)              (None, 17, 17, 16)    2320        leaky_re_lu_420[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_480 (BatchNo (None, 17, 17, 16)    64          conv2d_503[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_421 (LeakyReLU)      (None, 17, 17, 16)    0           batch_normalization_480[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_504 (Conv2D)              (None, 17, 17, 32)    544         leaky_re_lu_421[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_481 (BatchNo (None, 17, 17, 32)    128         conv2d_504[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "add_118 (Add)                    (None, 17, 17, 32)    0           leaky_re_lu_419[0][0]            \n",
      "                                                                   batch_normalization_481[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_422 (LeakyReLU)      (None, 17, 17, 32)    0           add_118[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_505 (Conv2D)              (None, 17, 17, 16)    528         leaky_re_lu_422[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_482 (BatchNo (None, 17, 17, 16)    64          conv2d_505[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_423 (LeakyReLU)      (None, 17, 17, 16)    0           batch_normalization_482[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_506 (Conv2D)              (None, 17, 17, 16)    2320        leaky_re_lu_423[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_483 (BatchNo (None, 17, 17, 16)    64          conv2d_506[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_424 (LeakyReLU)      (None, 17, 17, 16)    0           batch_normalization_483[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_507 (Conv2D)              (None, 17, 17, 32)    544         leaky_re_lu_424[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_484 (BatchNo (None, 17, 17, 32)    128         conv2d_507[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "add_119 (Add)                    (None, 17, 17, 32)    0           leaky_re_lu_422[0][0]            \n",
      "                                                                   batch_normalization_484[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_425 (LeakyReLU)      (None, 17, 17, 32)    0           add_119[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_508 (Conv2D)              (None, 17, 17, 16)    528         leaky_re_lu_425[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_485 (BatchNo (None, 17, 17, 16)    64          conv2d_508[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_426 (LeakyReLU)      (None, 17, 17, 16)    0           batch_normalization_485[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_509 (Conv2D)              (None, 17, 17, 16)    2320        leaky_re_lu_426[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_486 (BatchNo (None, 17, 17, 16)    64          conv2d_509[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_427 (LeakyReLU)      (None, 17, 17, 16)    0           batch_normalization_486[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_510 (Conv2D)              (None, 17, 17, 32)    544         leaky_re_lu_427[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_487 (BatchNo (None, 17, 17, 32)    128         conv2d_510[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "add_120 (Add)                    (None, 17, 17, 32)    0           leaky_re_lu_425[0][0]            \n",
      "                                                                   batch_normalization_487[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_428 (LeakyReLU)      (None, 17, 17, 32)    0           add_120[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_89 (MaxPooling2D)  (None, 8, 8, 32)      0           leaky_re_lu_428[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_511 (Conv2D)              (None, 8, 8, 32)      1056        max_pooling2d_89[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_488 (BatchNo (None, 8, 8, 32)      128         conv2d_511[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_429 (LeakyReLU)      (None, 8, 8, 32)      0           batch_normalization_488[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_512 (Conv2D)              (None, 8, 8, 32)      9248        leaky_re_lu_429[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_489 (BatchNo (None, 8, 8, 32)      128         conv2d_512[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_430 (LeakyReLU)      (None, 8, 8, 32)      0           batch_normalization_489[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_514 (Conv2D)              (None, 8, 8, 64)      2112        max_pooling2d_89[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_513 (Conv2D)              (None, 8, 8, 64)      2112        leaky_re_lu_430[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_491 (BatchNo (None, 8, 8, 64)      256         conv2d_514[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_490 (BatchNo (None, 8, 8, 64)      256         conv2d_513[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "add_121 (Add)                    (None, 8, 8, 64)      0           batch_normalization_491[0][0]    \n",
      "                                                                   batch_normalization_490[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_431 (LeakyReLU)      (None, 8, 8, 64)      0           add_121[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_515 (Conv2D)              (None, 8, 8, 32)      2080        leaky_re_lu_431[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_492 (BatchNo (None, 8, 8, 32)      128         conv2d_515[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_432 (LeakyReLU)      (None, 8, 8, 32)      0           batch_normalization_492[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_516 (Conv2D)              (None, 8, 8, 32)      9248        leaky_re_lu_432[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_493 (BatchNo (None, 8, 8, 32)      128         conv2d_516[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_433 (LeakyReLU)      (None, 8, 8, 32)      0           batch_normalization_493[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_517 (Conv2D)              (None, 8, 8, 64)      2112        leaky_re_lu_433[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_494 (BatchNo (None, 8, 8, 64)      256         conv2d_517[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "add_122 (Add)                    (None, 8, 8, 64)      0           leaky_re_lu_431[0][0]            \n",
      "                                                                   batch_normalization_494[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_434 (LeakyReLU)      (None, 8, 8, 64)      0           add_122[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_518 (Conv2D)              (None, 8, 8, 32)      2080        leaky_re_lu_434[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_495 (BatchNo (None, 8, 8, 32)      128         conv2d_518[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_435 (LeakyReLU)      (None, 8, 8, 32)      0           batch_normalization_495[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_519 (Conv2D)              (None, 8, 8, 32)      9248        leaky_re_lu_435[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_496 (BatchNo (None, 8, 8, 32)      128         conv2d_519[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_436 (LeakyReLU)      (None, 8, 8, 32)      0           batch_normalization_496[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_520 (Conv2D)              (None, 8, 8, 64)      2112        leaky_re_lu_436[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_497 (BatchNo (None, 8, 8, 64)      256         conv2d_520[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "add_123 (Add)                    (None, 8, 8, 64)      0           leaky_re_lu_434[0][0]            \n",
      "                                                                   batch_normalization_497[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_437 (LeakyReLU)      (None, 8, 8, 64)      0           add_123[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_521 (Conv2D)              (None, 8, 8, 32)      2080        leaky_re_lu_437[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_498 (BatchNo (None, 8, 8, 32)      128         conv2d_521[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_438 (LeakyReLU)      (None, 8, 8, 32)      0           batch_normalization_498[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_522 (Conv2D)              (None, 8, 8, 32)      9248        leaky_re_lu_438[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_499 (BatchNo (None, 8, 8, 32)      128         conv2d_522[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_439 (LeakyReLU)      (None, 8, 8, 32)      0           batch_normalization_499[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_523 (Conv2D)              (None, 8, 8, 64)      2112        leaky_re_lu_439[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_500 (BatchNo (None, 8, 8, 64)      256         conv2d_523[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "add_124 (Add)                    (None, 8, 8, 64)      0           leaky_re_lu_437[0][0]            \n",
      "                                                                   batch_normalization_500[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_440 (LeakyReLU)      (None, 8, 8, 64)      0           add_124[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_90 (MaxPooling2D)  (None, 4, 4, 64)      0           leaky_re_lu_440[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_524 (Conv2D)              (None, 2, 2, 64)      36928       max_pooling2d_90[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_501 (BatchNo (None, 2, 2, 64)      256         conv2d_524[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_441 (LeakyReLU)      (None, 2, 2, 64)      0           batch_normalization_501[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_30 (Glo (None, 64)            0           leaky_re_lu_441[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_80 (Dense)                 (None, 1)             65          global_average_pooling2d_30[0][0]\n",
      "====================================================================================================\n",
      "Total params: 115,041\n",
      "Trainable params: 113,121\n",
      "Non-trainable params: 1,920\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "\n",
    "#\n",
    "# image dimensions\n",
    "#\n",
    "\n",
    "img_height = 75\n",
    "img_width = 75\n",
    "img_channels = 3\n",
    "\n",
    "#\n",
    "# network params\n",
    "#\n",
    "\n",
    "cardinality = 1\n",
    "\n",
    "\n",
    "def residual_network(x):\n",
    "    \"\"\"\n",
    "    ResNeXt by default. For ResNet set `cardinality` = 1 above.\n",
    "    \n",
    "    \"\"\"\n",
    "    def add_common_layers(y):\n",
    "        y = layers.BatchNormalization()(y)\n",
    "        y = layers.LeakyReLU()(y)\n",
    "        #y = layers.Dropout(0.4)(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def grouped_convolution(y, nb_channels, _strides):\n",
    "        # when `cardinality` == 1 this is just a standard convolution\n",
    "        if cardinality == 1:\n",
    "            return layers.Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n",
    "        \n",
    "        assert not nb_channels % cardinality\n",
    "        _d = nb_channels // cardinality\n",
    "\n",
    "        # in a grouped convolution layer, input and output channels are divided into `cardinality` groups,\n",
    "        # and convolutions are separately performed within each group\n",
    "        groups = []\n",
    "        for j in range(cardinality):\n",
    "            group = layers.Lambda(lambda z: z[:, :, :, j * _d:j * _d + _d])(y)\n",
    "            groups.append(layers.Conv2D(_d, kernel_size=(3, 3), strides=_strides, padding='same')(group))\n",
    "            \n",
    "        # the grouped convolutional layer concatenates them as the outputs of the layer\n",
    "        y = layers.concatenate(groups)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def residual_block(y, nb_channels_in, nb_channels_out, _strides=(1, 1), _project_shortcut=False):\n",
    "        \"\"\"\n",
    "        Our network consists of a stack of residual blocks. These blocks have the same topology,\n",
    "        and are subject to two simple rules:\n",
    "        - If producing spatial maps of the same size, the blocks share the same hyper-parameters (width and filter sizes).\n",
    "        - Each time the spatial map is down-sampled by a factor of 2, the width of the blocks is multiplied by a factor of 2.\n",
    "        \"\"\"\n",
    "        shortcut = y\n",
    "\n",
    "        # we modify the residual building block as a bottleneck design to make the network more economical\n",
    "        y = layers.Conv2D(nb_channels_in, kernel_size=(1, 1), strides=(1, 1), padding='same')(y)\n",
    "        y = add_common_layers(y)\n",
    "\n",
    "        # ResNeXt (identical to ResNet when `cardinality` == 1)\n",
    "        y = grouped_convolution(y, nb_channels_in, _strides=_strides)\n",
    "        y = add_common_layers(y)\n",
    "\n",
    "        y = layers.Conv2D(nb_channels_out, kernel_size=(1, 1), strides=(1, 1), padding='same')(y)\n",
    "        # batch normalization is employed after aggregating the transformations and before adding to the shortcut\n",
    "        y = layers.BatchNormalization()(y)\n",
    "\n",
    "        # identity shortcuts used directly when the input and output are of the same dimensions\n",
    "        if _project_shortcut or _strides != (1, 1):\n",
    "            # when the dimensions increase projection shortcut is used to match dimensions (done by 1×1 convolutions)\n",
    "            # when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2\n",
    "            shortcut = layers.Conv2D(nb_channels_out, kernel_size=(1, 1), strides=_strides, padding='same')(shortcut)\n",
    "            shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "        y = layers.add([shortcut, y])\n",
    "\n",
    "        # relu is performed right after each batch normalization,\n",
    "        # expect for the output of the block where relu is performed after the adding to the shortcut\n",
    "        y = layers.LeakyReLU()(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "    # conv1\n",
    "    x = layers.Conv2D(32, kernel_size=(7, 7), strides=(2, 2))(x)\n",
    "    x = add_common_layers(x)\n",
    "    \n",
    "    \n",
    "    # conv2\n",
    "    x = layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "    for i in range(4):\n",
    "        project_shortcut = True if i == 0 else False\n",
    "        x = residual_block(x, 16, 32, _project_shortcut=project_shortcut)\n",
    "        \n",
    "    # conv2\n",
    "    x = layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "    for i in range(4):\n",
    "        project_shortcut = True if i == 0 else False\n",
    "        x = residual_block(x, 32, 64, _project_shortcut=project_shortcut)\n",
    "    \n",
    "    x = layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "    x = layers.Conv2D(64, kernel_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = add_common_layers(x)\n",
    "    \n",
    "\n",
    "\n",
    "    #x = layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "    #for i in range(2):\n",
    "    #    project_shortcut = True if i == 0 else False\n",
    "    #    x = residual_block(x, 64, 128, _project_shortcut=project_shortcut)\n",
    "\n",
    "\n",
    "    # conv3\n",
    "    #for i in range(4):\n",
    "        # down-sampling is performed by conv3_1, conv4_1, and conv5_1 with a stride of 2\n",
    "     #   strides = (2, 2) if i == 0 else (1, 1)\n",
    "    #    x = residual_block(x, 256, 512, _strides=strides)\n",
    "\n",
    "    # conv4\n",
    "    #for i in range(6):\n",
    "    #    strides = (2, 2) if i == 0 else (1, 1)\n",
    "    #    x = residual_block(x, 512, 1024, _strides=strides)\n",
    "\n",
    "    ## conv5\n",
    "    #for i in range(3):\n",
    "     #   strides = (2, 2) if i == 0 else (1, 1)\n",
    "    #    x = residual_block(x, 1024, 2048, _strides=strides)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(1, activation='sigmoid')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "image_tensor = layers.Input(shape=(img_height, img_width, img_channels))\n",
    "network_output = residual_network(image_tensor)\n",
    "  \n",
    "model = models.Model(inputs=[image_tensor], outputs=[network_output])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "248/248 [==============================] - 50s - loss: 0.2708 - mean_absolute_error: 0.1698 - val_loss: 2.0550 - val_mean_absolute_error: 0.5244\n",
      "Epoch 2/80\n",
      "248/248 [==============================] - 44s - loss: 0.1483 - mean_absolute_error: 0.0903 - val_loss: 2.2274 - val_mean_absolute_error: 0.5357\n",
      "Epoch 3/80\n",
      "248/248 [==============================] - 72s - loss: 0.0959 - mean_absolute_error: 0.0575 - val_loss: 0.4138 - val_mean_absolute_error: 0.1869\n",
      "Epoch 4/80\n",
      "248/248 [==============================] - 44s - loss: 0.0638 - mean_absolute_error: 0.0380 - val_loss: 0.9665 - val_mean_absolute_error: 0.2194\n",
      "Epoch 5/80\n",
      "248/248 [==============================] - 45s - loss: 0.0419 - mean_absolute_error: 0.0248 - val_loss: 0.8404 - val_mean_absolute_error: 0.1762\n",
      "Epoch 6/80\n",
      "248/248 [==============================] - 45s - loss: 0.0354 - mean_absolute_error: 0.0204 - val_loss: 0.7721 - val_mean_absolute_error: 0.1425\n",
      "Epoch 7/80\n",
      "248/248 [==============================] - 45s - loss: 0.0293 - mean_absolute_error: 0.0166 - val_loss: 0.5653 - val_mean_absolute_error: 0.1193\n",
      "Epoch 8/80\n",
      "248/248 [==============================] - 45s - loss: 0.0258 - mean_absolute_error: 0.0144 - val_loss: 2.0486 - val_mean_absolute_error: 0.3000\n",
      "Epoch 9/80\n",
      "248/248 [==============================] - 45s - loss: 0.0188 - mean_absolute_error: 0.0109 - val_loss: 0.6405 - val_mean_absolute_error: 0.1283\n",
      "Epoch 10/80\n",
      "248/248 [==============================] - 45s - loss: 0.0181 - mean_absolute_error: 0.0103 - val_loss: 0.6715 - val_mean_absolute_error: 0.1382\n",
      "Epoch 11/80\n",
      "248/248 [==============================] - 45s - loss: 0.0146 - mean_absolute_error: 0.0084 - val_loss: 0.6259 - val_mean_absolute_error: 0.1194\n",
      "Epoch 12/80\n",
      "248/248 [==============================] - 45s - loss: 0.0160 - mean_absolute_error: 0.0086 - val_loss: 0.6507 - val_mean_absolute_error: 0.1168\n",
      "Epoch 13/80\n",
      "248/248 [==============================] - 45s - loss: 0.0120 - mean_absolute_error: 0.0068 - val_loss: 0.8458 - val_mean_absolute_error: 0.1410\n",
      "Epoch 14/80\n",
      "248/248 [==============================] - 45s - loss: 0.0112 - mean_absolute_error: 0.0063 - val_loss: 0.7750 - val_mean_absolute_error: 0.1325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7ca9839ed0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 264\n",
    "epochs = 80\n",
    "steps_per_epoch=2**16/batch_size #131072 samples\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.adam(lr=0.01, decay=0.005), metrics=['mae'])\n",
    "model.fit_generator(training_set, steps_per_epoch=steps_per_epoch, epochs=epochs, \n",
    "                    callbacks=callbacks, validation_data=valid_set, validation_steps=steps_per_epoch/10)\n",
    "\n",
    "print(model.evaluate_generator(training_set, steps_per_epoch))\n",
    "print(model.evaluate_generator(valid_set, steps_per_epoch/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1203 samples, validate on 401 samples\n",
      "Epoch 1/80\n",
      "1203/1203 [==============================] - 6s - loss: 0.5884 - mean_absolute_error: 0.3730 - val_loss: 8.3605 - val_mean_absolute_error: 0.5187\n",
      "Epoch 2/80\n",
      "1203/1203 [==============================] - 1s - loss: 0.3766 - mean_absolute_error: 0.2469 - val_loss: 8.3605 - val_mean_absolute_error: 0.5187\n",
      "Epoch 3/80\n",
      "1203/1203 [==============================] - 1s - loss: 0.3273 - mean_absolute_error: 0.2052 - val_loss: 7.4735 - val_mean_absolute_error: 0.5071\n",
      "Epoch 4/80\n",
      "1203/1203 [==============================] - 2s - loss: 0.3077 - mean_absolute_error: 0.1835 - val_loss: 7.6541 - val_mean_absolute_error: 0.5183\n",
      "Epoch 5/80\n",
      "1203/1203 [==============================] - 1s - loss: 0.2680 - mean_absolute_error: 0.1742 - val_loss: 0.6835 - val_mean_absolute_error: 0.3062\n",
      "Epoch 6/80\n",
      "1203/1203 [==============================] - 1s - loss: 0.2227 - mean_absolute_error: 0.1384 - val_loss: 0.4018 - val_mean_absolute_error: 0.2089\n",
      "Epoch 7/80\n",
      "1203/1203 [==============================] - 1s - loss: 0.1718 - mean_absolute_error: 0.1150 - val_loss: 0.3341 - val_mean_absolute_error: 0.1573\n",
      "Epoch 8/80\n",
      "1203/1203 [==============================] - 1s - loss: 0.1702 - mean_absolute_error: 0.1030 - val_loss: 0.4922 - val_mean_absolute_error: 0.1937\n",
      "Epoch 9/80\n",
      "1203/1203 [==============================] - 1s - loss: 0.1435 - mean_absolute_error: 0.0875 - val_loss: 0.7012 - val_mean_absolute_error: 0.2478\n",
      "Epoch 10/80\n",
      "1203/1203 [==============================] - 1s - loss: 0.1310 - mean_absolute_error: 0.0815 - val_loss: 1.0883 - val_mean_absolute_error: 0.2084\n",
      "Epoch 11/80\n",
      "1203/1203 [==============================] - 1s - loss: 0.1106 - mean_absolute_error: 0.0732 - val_loss: 1.2487 - val_mean_absolute_error: 0.2579\n",
      "Epoch 12/80\n",
      "1203/1203 [==============================] - 1s - loss: 0.0847 - mean_absolute_error: 0.0505 - val_loss: 0.8159 - val_mean_absolute_error: 0.2038\n",
      "Epoch 13/80\n",
      "1203/1203 [==============================] - 1s - loss: 0.0751 - mean_absolute_error: 0.0511 - val_loss: 0.5102 - val_mean_absolute_error: 0.1731\n",
      "Epoch 14/80\n",
      "1203/1203 [==============================] - 1s - loss: 0.0503 - mean_absolute_error: 0.0332 - val_loss: 0.8930 - val_mean_absolute_error: 0.2223\n",
      "Epoch 15/80\n",
      "1203/1203 [==============================] - 2s - loss: 0.0402 - mean_absolute_error: 0.0280 - val_loss: 0.7296 - val_mean_absolute_error: 0.1760\n",
      "Epoch 16/80\n",
      "1203/1203 [==============================] - 1s - loss: 0.0504 - mean_absolute_error: 0.0284 - val_loss: 0.5706 - val_mean_absolute_error: 0.1642\n",
      "Epoch 17/80\n",
      "1203/1203 [==============================] - 1s - loss: 0.0492 - mean_absolute_error: 0.0285 - val_loss: 0.6755 - val_mean_absolute_error: 0.1580\n",
      "Epoch 18/80\n",
      "1203/1203 [==============================] - 1s - loss: 0.0217 - mean_absolute_error: 0.0161 - val_loss: 0.6407 - val_mean_absolute_error: 0.1603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7d5a18e4d0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 80\n",
    "steps_per_epoch=2**16/batch_size #131072 samples\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.adam(lr=0.01, decay=0.005), metrics=['mae'])\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size = 32, \n",
    "                    callbacks=callbacks, validation_data=[X_valid, y_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>is_iceberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5941774d</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4023181e</td>\n",
       "      <td>0.002718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b20200e4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e7f018bb</td>\n",
       "      <td>0.990814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4371c8c3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a8d9b1fd</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29e7727e</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>92a51ffb</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>c769ac97</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>aee0547d</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id is_iceberg\n",
       "0  5941774d   0.000000\n",
       "1  4023181e   0.002718\n",
       "2  b20200e4   0.000000\n",
       "3  e7f018bb   0.990814\n",
       "4  4371c8c3   0.000000\n",
       "5  a8d9b1fd   0.000000\n",
       "6  29e7727e   0.000007\n",
       "7  92a51ffb   0.999999\n",
       "8  c769ac97   0.000000\n",
       "9  aee0547d   0.000006"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions = model.predict_generator(test_set, steps = test.shape[0])\n",
    "test_predictions = test_predictions.reshape((test_predictions.shape[0]))\n",
    "test_predictions = ['{:f}'.format(item) for item in test_predictions]\n",
    "\n",
    "submission = pd.DataFrame({'id': test[\"id\"], 'is_iceberg': test_predictions})\n",
    "submission.to_csv('./submitions/sub132.csv', index = False)\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_568 (Conv2D)          (None, 75, 75, 16)        304       \n",
      "_________________________________________________________________\n",
      "batch_normalization_546 (Bat (None, 75, 75, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_480 (LeakyReLU)  (None, 75, 75, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_123 (MaxPoolin (None, 37, 37, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_569 (Conv2D)          (None, 37, 37, 32)        12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_547 (Bat (None, 37, 37, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_481 (LeakyReLU)  (None, 37, 37, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_124 (MaxPoolin (None, 18, 18, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_570 (Conv2D)          (None, 18, 18, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_548 (Bat (None, 18, 18, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_482 (LeakyReLU)  (None, 18, 18, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_125 (MaxPoolin (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_571 (Conv2D)          (None, 9, 9, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_549 (Bat (None, 9, 9, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_483 (LeakyReLU)  (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_126 (MaxPoolin (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_572 (Conv2D)          (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_550 (Bat (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_484 (LeakyReLU)  (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_127 (MaxPoolin (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_573 (Conv2D)          (None, 2, 2, 32)          18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_551 (Bat (None, 2, 2, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_485 (LeakyReLU)  (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_11 (Glo (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 125,073\n",
      "Trainable params: 124,529\n",
      "Non-trainable params: 544\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train_gen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        zoom_range = 0.1,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        vertical_flip=True,\n",
    "        horizontal_flip=True,\n",
    "        #rescale=1./255,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "\n",
    "test_gen = ImageDataGenerator()\n",
    "\n",
    "\n",
    "training_set = train_gen.flow(X_train, y_train, batch_size=32)\n",
    "\n",
    "\n",
    "#model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, kernel_size=(3,3), padding='same', input_shape=(75,75,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(keras.layers.LeakyReLU())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(5,5), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(keras.layers.LeakyReLU())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(keras.layers.LeakyReLU())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(keras.layers.LeakyReLU())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(keras.layers.LeakyReLU())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(keras.layers.LeakyReLU())\n",
    "#model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "model.add(GlobalMaxPool2D())\n",
    "\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "512/512 [==============================] - 184s - loss: 0.3179 - acc: 0.6838 - val_loss: 0.3598 - val_acc: 0.6417\n",
      "Epoch 2/80\n",
      "512/512 [==============================] - 14s - loss: 0.2125 - acc: 0.7883 - val_loss: 0.2101 - val_acc: 0.7913\n",
      "Epoch 3/80\n",
      "512/512 [==============================] - 14s - loss: 0.1964 - acc: 0.8040 - val_loss: 0.2063 - val_acc: 0.7975\n",
      "Epoch 4/80\n",
      "512/512 [==============================] - 14s - loss: 0.1752 - acc: 0.8261 - val_loss: 0.2740 - val_acc: 0.7570\n",
      "Epoch 5/80\n",
      "512/512 [==============================] - 14s - loss: 0.1806 - acc: 0.8203 - val_loss: 0.3265 - val_acc: 0.6667\n",
      "Epoch 6/80\n",
      "512/512 [==============================] - 14s - loss: 0.1593 - acc: 0.8409 - val_loss: 0.1467 - val_acc: 0.8536\n",
      "Epoch 7/80\n",
      "512/512 [==============================] - 14s - loss: 0.1558 - acc: 0.8444 - val_loss: 0.1699 - val_acc: 0.8349\n",
      "Epoch 8/80\n",
      "512/512 [==============================] - 14s - loss: 0.1674 - acc: 0.8325 - val_loss: 0.1456 - val_acc: 0.8567\n",
      "Epoch 9/80\n",
      "512/512 [==============================] - 14s - loss: 0.1529 - acc: 0.8472 - val_loss: 0.2632 - val_acc: 0.7352\n",
      "Epoch 10/80\n",
      "512/512 [==============================] - ETA: 0s - loss: 0.1449 - acc: 0.855 - 15s - loss: 0.1451 - acc: 0.8549 - val_loss: 0.1795 - val_acc: 0.8193\n",
      "Epoch 11/80\n",
      "512/512 [==============================] - 14s - loss: 0.1509 - acc: 0.8487 - val_loss: 0.2016 - val_acc: 0.8006\n",
      "Epoch 12/80\n",
      "512/512 [==============================] - 14s - loss: 0.1502 - acc: 0.8501 - val_loss: 0.1387 - val_acc: 0.8598\n",
      "Epoch 13/80\n",
      "512/512 [==============================] - 14s - loss: 0.1775 - acc: 0.8227 - val_loss: 0.1839 - val_acc: 0.8193\n",
      "Epoch 14/80\n",
      "512/512 [==============================] - 14s - loss: 0.1540 - acc: 0.8464 - val_loss: 0.1532 - val_acc: 0.8442\n",
      "Epoch 15/80\n",
      "512/512 [==============================] - 14s - loss: 0.1468 - acc: 0.8535 - val_loss: 0.1492 - val_acc: 0.8536\n",
      "Epoch 16/80\n",
      "512/512 [==============================] - 14s - loss: 0.1477 - acc: 0.8526 - val_loss: 0.1700 - val_acc: 0.8255\n",
      "Epoch 17/80\n",
      "512/512 [==============================] - 14s - loss: 0.1500 - acc: 0.8496 - val_loss: 0.1619 - val_acc: 0.8349\n",
      "Epoch 18/80\n",
      "512/512 [==============================] - 14s - loss: 0.1514 - acc: 0.8487 - val_loss: 0.1637 - val_acc: 0.8349\n",
      "[0.14199758411738095, 0.85788081138969385]\n",
      "320/321 [============================>.] - ETA: 0s[0.1494131429915859, 0.84735202492211836]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 80\n",
    "steps_per_epoch=2**14/batch_size \n",
    "\n",
    "callbacks = get_callbacks('./model.hdf5', patience=5)\n",
    "\n",
    "model.compile(loss='mae', optimizer=keras.optimizers.adam(lr=0.01, decay=5e-5), metrics=['accuracy'])\n",
    "model.fit_generator(training_set, steps_per_epoch=steps_per_epoch, epochs=epochs, \n",
    "                    callbacks=callbacks, validation_data=[X_valid, y_valid])\n",
    "\n",
    "print(model.evaluate_generator(training_set, steps_per_epoch))\n",
    "print(model.evaluate(X_testing, y_testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 962 samples, validate on 321 samples\n",
      "Epoch 1/80\n",
      "962/962 [==============================] - 6s - loss: 0.3720 - mean_absolute_error: 0.3720 - val_loss: 0.5047 - val_mean_absolute_error: 0.5047\n",
      "Epoch 2/80\n",
      "962/962 [==============================] - 1s - loss: 0.2628 - mean_absolute_error: 0.2628 - val_loss: 0.5047 - val_mean_absolute_error: 0.5047\n",
      "Epoch 3/80\n",
      "962/962 [==============================] - 0s - loss: 0.1916 - mean_absolute_error: 0.1916 - val_loss: 0.4925 - val_mean_absolute_error: 0.4925\n",
      "Epoch 4/80\n",
      "962/962 [==============================] - 1s - loss: 0.2413 - mean_absolute_error: 0.2413 - val_loss: 0.4904 - val_mean_absolute_error: 0.4904\n",
      "Epoch 5/80\n",
      "962/962 [==============================] - 1s - loss: 0.1804 - mean_absolute_error: 0.1804 - val_loss: 0.4667 - val_mean_absolute_error: 0.4667\n",
      "Epoch 6/80\n",
      "962/962 [==============================] - 1s - loss: 0.1552 - mean_absolute_error: 0.1552 - val_loss: 0.5047 - val_mean_absolute_error: 0.5047\n",
      "Epoch 7/80\n",
      "962/962 [==============================] - 1s - loss: 0.1358 - mean_absolute_error: 0.1358 - val_loss: 0.1680 - val_mean_absolute_error: 0.1680\n",
      "Epoch 8/80\n",
      "962/962 [==============================] - 1s - loss: 0.1358 - mean_absolute_error: 0.1358 - val_loss: 0.2302 - val_mean_absolute_error: 0.2302\n",
      "Epoch 9/80\n",
      "962/962 [==============================] - 1s - loss: 0.1570 - mean_absolute_error: 0.1570 - val_loss: 0.3995 - val_mean_absolute_error: 0.3995\n",
      "Epoch 10/80\n",
      "962/962 [==============================] - 0s - loss: 0.1609 - mean_absolute_error: 0.1609 - val_loss: 0.3649 - val_mean_absolute_error: 0.3649\n",
      "Epoch 11/80\n",
      "962/962 [==============================] - 1s - loss: 0.1459 - mean_absolute_error: 0.1459 - val_loss: 0.1655 - val_mean_absolute_error: 0.1655\n",
      "Epoch 12/80\n",
      "962/962 [==============================] - 1s - loss: 0.1161 - mean_absolute_error: 0.1161 - val_loss: 0.1949 - val_mean_absolute_error: 0.1949\n",
      "Epoch 13/80\n",
      "962/962 [==============================] - 1s - loss: 0.1105 - mean_absolute_error: 0.1105 - val_loss: 0.2235 - val_mean_absolute_error: 0.2235\n",
      "Epoch 14/80\n",
      "962/962 [==============================] - 1s - loss: 0.1345 - mean_absolute_error: 0.1345 - val_loss: 0.1673 - val_mean_absolute_error: 0.1673\n",
      "Epoch 15/80\n",
      "962/962 [==============================] - 1s - loss: 0.1323 - mean_absolute_error: 0.1323 - val_loss: 0.4366 - val_mean_absolute_error: 0.4366\n",
      "Epoch 16/80\n",
      "962/962 [==============================] - 1s - loss: 0.1367 - mean_absolute_error: 0.1367 - val_loss: 0.1871 - val_mean_absolute_error: 0.1871\n",
      "Epoch 17/80\n",
      "962/962 [==============================] - 1s - loss: 0.1046 - mean_absolute_error: 0.1046 - val_loss: 0.1864 - val_mean_absolute_error: 0.1864\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7f037d5e50>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 80\n",
    "steps_per_epoch=2**16/batch_size #131072 samples\n",
    "\n",
    "model.compile(loss='mae', optimizer=keras.optimizers.adam(lr=0.01, decay=5e-5), metrics=['mae'])\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size = 32, \n",
    "                    callbacks=callbacks, validation_data=[X_valid, y_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "960/962 [============================>.] - ETA: 0s[0.14759869974207235, 0.14759869974207235]\n",
      "288/321 [=========================>....] - ETA: 0s[0.17678908915534569, 0.17678908915534569]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(X_train, y_train))\n",
    "print(model.evaluate(X_testing, y_testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>is_iceberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5941774d</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4023181e</td>\n",
       "      <td>0.999976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b20200e4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e7f018bb</td>\n",
       "      <td>0.999727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4371c8c3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a8d9b1fd</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29e7727e</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>92a51ffb</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>c769ac97</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>aee0547d</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id is_iceberg\n",
       "0  5941774d   0.000000\n",
       "1  4023181e   0.999976\n",
       "2  b20200e4   0.000000\n",
       "3  e7f018bb   0.999727\n",
       "4  4371c8c3   0.000000\n",
       "5  a8d9b1fd   0.999999\n",
       "6  29e7727e   0.000000\n",
       "7  92a51ffb   1.000000\n",
       "8  c769ac97   0.000000\n",
       "9  aee0547d   0.000000"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions = model.predict(X_test)\n",
    "test_predictions = test_predictions.reshape((test_predictions.shape[0]))\n",
    "test_predictions = ['{:f}'.format(item) for item in test_predictions]\n",
    "\n",
    "submission = pd.DataFrame({'id': test[\"id\"], 'is_iceberg': test_predictions})\n",
    "submission.to_csv('./submitions/sub149.csv', index = False)\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8424, 1)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
