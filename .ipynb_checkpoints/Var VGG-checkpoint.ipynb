{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import datetime as dt\n",
    "import cv2\n",
    "import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data loaded\n",
      "Test data loaded\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:34: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGBs Done\n"
     ]
    }
   ],
   "source": [
    "data_dir = './' #Change to your directory here\n",
    "\n",
    "def load_data(data_dir):\n",
    "    train = pd.read_json(data_dir + 'train.json')\n",
    "    print(\"Train data loaded\")\n",
    "    test = pd.read_json(data_dir + 'test.json')\n",
    "    print(\"Test data loaded\")\n",
    "    #Fill 'na' angles with mode\n",
    "    train.inc_angle = train.inc_angle.replace('na', 0)\n",
    "    train.inc_angle = train.inc_angle.astype(float).fillna(0.0)\n",
    "    test.inc_angle = test.inc_angle.replace('na', 0)\n",
    "    test.inc_angle = test.inc_angle.astype(float).fillna(0.0)\n",
    "    return train, test\n",
    "\n",
    "train, test = load_data(data_dir)\n",
    "print(\"done\")\n",
    "\n",
    "X_angle_train = np.array(train.inc_angle)\n",
    "X_angle_test = np.array(test.inc_angle)\n",
    "\n",
    "\n",
    "def color_composite(data):\n",
    "    w,h = 75,75\n",
    "    rgb_arrays = []\n",
    "    for i, row in data.iterrows():\n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        band_3 = band_1 * row['inc_angle']\n",
    "        band_4 = band_2 * row['inc_angle']\n",
    "\n",
    "\n",
    "        r = (band_1 + abs(band_1.min())) / np.max((band_1 + abs(band_1.min())))\n",
    "        g = (band_2 + abs(band_2.min())) / np.max((band_2 + abs(band_2.min())))\n",
    "        b = (band_3 + abs(band_3.min())) / np.max((band_3 + abs(band_3.min())))\n",
    "        a = (band_3 + abs(band_3.min())) / np.max((band_3 + abs(band_3.min())))\n",
    "\n",
    "        \n",
    "        rgba = np.dstack((r, g))\n",
    "\n",
    "        #rgba = cv2.resize(rgba, (w,h)).astype(np.float32)\n",
    "        \n",
    "        rgb_arrays.append(rgba)\n",
    "    return np.array(rgb_arrays)\n",
    "\n",
    "rgb_train = color_composite(train)\n",
    "rgb_test = color_composite(test)\n",
    "print(\"RGBs Done\")\n",
    "\n",
    "y_train = np.array(train['is_iceberg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1363, 75, 75, 2)\n",
      "X_valid: (241, 75, 75, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, X_angle_train, X_angle_valid, y_train, y_valid = train_test_split(rgb_train,\n",
    "                                    X_angle_train, y_train, random_state=420, test_size=0.15)\n",
    "\n",
    "print(\"X_train:\",X_train.shape)\n",
    "print(\"X_valid:\",X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img_inputs (InputLayer)      (None, 75, 75, 2)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_127 (Bat (None, 75, 75, 2)         8         \n",
      "_________________________________________________________________\n",
      "conv2d_107 (Conv2D)          (None, 75, 75, 32)        608       \n",
      "_________________________________________________________________\n",
      "batch_normalization_129 (Bat (None, 75, 75, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_59 (LeakyReLU)   (None, 75, 75, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 37, 37, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 37, 37, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_108 (Conv2D)          (None, 37, 37, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_130 (Bat (None, 37, 37, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_60 (LeakyReLU)   (None, 37, 37, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 18, 18, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_109 (Conv2D)          (None, 18, 18, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_131 (Bat (None, 18, 18, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_61 (LeakyReLU)   (None, 18, 18, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 18, 18, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_110 (Conv2D)          (None, 18, 18, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_132 (Bat (None, 18, 18, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_62 (LeakyReLU)   (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_111 (Conv2D)          (None, 18, 18, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_133 (Bat (None, 18, 18, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_63 (LeakyReLU)   (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_112 (Conv2D)          (None, 18, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_134 (Bat (None, 18, 18, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_64 (LeakyReLU)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_113 (Conv2D)          (None, 18, 18, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_135 (Bat (None, 18, 18, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_65 (LeakyReLU)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_114 (Conv2D)          (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_136 (Bat (None, 18, 18, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_66 (LeakyReLU)   (None, 18, 18, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_115 (Conv2D)          (None, 18, 18, 128)       589952    \n",
      "_________________________________________________________________\n",
      "batch_normalization_137 (Bat (None, 18, 18, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_67 (LeakyReLU)   (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_116 (Conv2D)          (None, 6, 6, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_138 (Bat (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_68 (LeakyReLU)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_117 (Conv2D)          (None, 2, 2, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_139 (Bat (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_69 (LeakyReLU)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_6 (Glob (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_140 (Bat (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_141 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 4,814,825\n",
      "Trainable params: 4,808,613\n",
      "Non-trainable params: 6,212\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def conv_block(x, nf=8, k=3, s=1, nb=2, p_act='elu'):\n",
    "    \n",
    "    for i in range(nb):\n",
    "        x = Conv2D(filters=nf, kernel_size=(k, k), strides=(s, s),  \n",
    "                   padding='same', kernel_initializer='he_uniform')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = keras.layers.LeakyReLU()(x)\n",
    "        \n",
    "    return x\n",
    "\n",
    "def res_block(x, nf=64, k=3, s=1, nb=1):\n",
    "    \n",
    "    for i in range(nb):\n",
    "        x = Conv2D(filters=nf, kernel_size=(k, k), strides=(s, s),  \n",
    "                   padding='same', kernel_initializer='he_uniform')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = keras.layers.LeakyReLU()(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        nf*=2\n",
    "        x = Conv2D(filters=nf, kernel_size=(k, k), strides=(s, s),  \n",
    "                   padding='same', kernel_initializer='he_uniform')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = keras.layers.LeakyReLU()(x)\n",
    "        \n",
    "    return x\n",
    "\n",
    "def dense_block(x, h=32, d=0.5, m=0., p_act='elu'):\n",
    "    return Dropout(d) (BatchNormalization(momentum=m) (Dense(h, activation=p_act)(x)))\n",
    "\n",
    "\n",
    "def bn_pooling(x, k=2, s=2, m=0): \n",
    "    return MaxPooling2D((k, k), strides=(s, s))(x)\n",
    "\n",
    "def get_model_with_res(img_shape=(75, 75, 2), num_classes=1, f=8, h=512):\n",
    "     \n",
    "    #model\n",
    "    bn_model = 0.99\n",
    "    p_activation = 'elu'\n",
    "    \n",
    "    #\n",
    "    input_img = Input(shape=img_shape, name='img_inputs')\n",
    "    input_img_bn = BatchNormalization(momentum=bn_model)(input_img)\n",
    "    \n",
    "    #\n",
    "    input_meta = Input(shape=[1], name='angle')\n",
    "    input_meta_bn = BatchNormalization(momentum=bn_model)(input_meta)\n",
    "\n",
    "    \n",
    "    #img_1\n",
    "    #img_1:block_1  8\n",
    "    f=32\n",
    "    img_1 = conv_block(input_img_bn, nf=f, k=3, s=1, nb=1,p_act=p_activation)\n",
    "    img_1 = bn_pooling(img_1, k=2, s=2, m=bn_model)\n",
    "    \n",
    "    #img_1:block_2\n",
    "    f=64\n",
    "    img_1 = Dropout(0.2)(img_1)\n",
    "    img_1 = conv_block(img_1, nf=f, k=3, s=1, nb=1,p_act=p_activation)\n",
    "    img_1 = bn_pooling(img_1, k=2, s=2, m=bn_model) \n",
    "    \n",
    "    #img_1:block_2\n",
    "    f=64\n",
    "    img_1 = res_block(img_1, nf=f, k=3, s=1, nb=3)\n",
    "\n",
    "    \n",
    "    #img_1:block_3\n",
    "    f=128\n",
    "\n",
    "    img_1 = conv_block(img_1, nf=f, k=3, s=1, nb=1,p_act=p_activation)\n",
    "    img_1 = bn_pooling(img_1, k=2, s=3, m=bn_model)\n",
    "    img_1 = Dropout(0.2)(img_1)\n",
    "    f=256\n",
    "    img_1 = conv_block(img_1, nf=f, k=3, s=1, nb=1,p_act=p_activation)\n",
    "    img_1 = bn_pooling(img_1, k=2, s=3, m=bn_model)\n",
    "    img_1 = Dropout(0.2)(img_1)\n",
    "    f=512\n",
    "    img_1 = conv_block(img_1, nf=f, k=3, s=1, nb=1,p_act=p_activation)\n",
    "    img_1 = bn_pooling(img_1, k=2, s=3, m=bn_model)\n",
    "    img_1 = Dropout(0.2)(img_1)    \n",
    "    img_1 = GlobalMaxPooling2D()(img_1)\n",
    "    \n",
    "    #full connect\n",
    "    concat = (Concatenate()([img_1, input_meta_bn]))\n",
    "    x = dense_block(img_1, h=h)\n",
    "    x = dense_block(x, h=256)\n",
    "    output = Dense(num_classes, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model([input_img, input_meta], output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_model(img_shape=(75, 75, 2), num_classes=1, f=8, h=512):\n",
    "\n",
    "    \"\"\"\n",
    "    img_shape: dimension for input image\n",
    "    f: filters of first conv blocks and generate filters in the following \n",
    "       blocks acorrdingly \n",
    "    h: units in dense hidden layer\n",
    "    \"\"\" \n",
    "    \n",
    "    #model\n",
    "    bn_model = 0.99\n",
    "    p_activation = 'elu'\n",
    "    \n",
    "    #\n",
    "    input_img = Input(shape=img_shape, name='img_inputs')\n",
    "    input_img_bn = BatchNormalization(momentum=bn_model)(input_img)\n",
    "    \n",
    "    #\n",
    "    input_meta = Input(shape=[1], name='angle')\n",
    "    input_meta_bn = BatchNormalization(momentum=bn_model)(input_meta)\n",
    "\n",
    "    \n",
    "    #img_1\n",
    "    #img_1:block_1  8\n",
    "    img_1 = conv_block(input_img_bn, nf=f, k=3, s=1, nb=6,p_act=p_activation)\n",
    "    img_1 = bn_pooling(img_1, k=2, s=2, m=bn_model)\n",
    "    \n",
    "    #img_1:block_2\n",
    "    f=16\n",
    "    img_1 = Dropout(0.2)(img_1)\n",
    "    img_1 = conv_block(img_1, nf=f, k=3, s=1, nb=6,p_act=p_activation)\n",
    "    img_1 = bn_pooling(img_1, k=2, s=2, m=bn_model) \n",
    "    \n",
    "    #img_1:block_3\n",
    "    f=32\n",
    "    img_1 = Dropout(0.2)(img_1)\n",
    "    img_1 = conv_block(img_1, nf=f, k=3, s=1, nb=6,p_act=p_activation)\n",
    "    img_1 = bn_pooling(img_1, k=2, s=3, m=bn_model)\n",
    "    \n",
    "    #img_1:block_4\n",
    "    f=64\n",
    "    img_1 = Dropout(0.2)(img_1)\n",
    "    img_1 = conv_block(img_1, nf=f, k=3, s=1, nb=6,p_act=p_activation)\n",
    "    img_1 = Dropout(0.2)(img_1)\n",
    "    img_1 = BatchNormalization(momentum=bn_model)(GlobalMaxPooling2D()(img_1))\n",
    "    \n",
    "    #img 2 #32  | tot_layers: 6\n",
    "    f = 8\n",
    "    img_2 = conv_block(input_img_bn, nf=f, k=3, s=1, nb=6,p_act=p_activation)\n",
    "    img_2 = bn_pooling(img_2, k=2, s=3, m=bn_model)\n",
    "    \n",
    "    #img_1:block_2\n",
    "    f=16\n",
    "    img_2 = Dropout(0.2)(img_2)\n",
    "    img_2 = conv_block(img_2, nf=f, k=3, s=1, nb=6,p_act=p_activation)\n",
    "    img_2 = bn_pooling(img_2, k=2, s=2, m=bn_model) \n",
    "    \n",
    "    #img_1:block_3\n",
    "    f=32\n",
    "    img_2 = Dropout(0.2)(img_2)\n",
    "    img_2 = conv_block(img_2, nf=f, k=3, s=1, nb=6,p_act=p_activation)\n",
    "    img_2 = bn_pooling(img_2, k=2, s=3, m=bn_model)\n",
    "    \n",
    "    #img_1:block_4\n",
    "    f=64\n",
    "    img_2 = Dropout(0.2)(img_2)\n",
    "    img_2 = conv_block(img_2, nf=f, k=3, s=1, nb=6,p_act=p_activation)\n",
    "    img_2 = Dropout(0.2)(img_2)\n",
    "    img_2 = BatchNormalization(momentum=bn_model)(GlobalMaxPooling2D()(img_2))\n",
    "    \n",
    "    \n",
    "    #full connect\n",
    "    concat = (Concatenate()([img_1, img_2, input_meta_bn]))\n",
    "    x = dense_block(img_1, h=h)\n",
    "    x = dense_block(x, h=h)\n",
    "    output = Dense(num_classes, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model([input_img, input_meta], output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "#model = get_model(img_shape=(75,75,2))\n",
    "model = get_model_with_res(img_shape=(75,75,2))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\preprocessing\\image.py:855: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (1363, 75, 75, 2) (2 channels).\n",
      "  ' (' + str(self.x.shape[channels_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 104s 202ms/step - loss: 0.5303 - acc: 0.7578 - val_loss: 0.3285 - val_acc: 0.8174\n",
      "Epoch 2/500\n",
      "512/512 [==============================] - 97s 189ms/step - loss: 0.4129 - acc: 0.8090 - val_loss: 0.4037 - val_acc: 0.7801\n",
      "Epoch 3/500\n",
      "512/512 [==============================] - 96s 188ms/step - loss: 0.3751 - acc: 0.8254 - val_loss: 0.3325 - val_acc: 0.8216\n",
      "Epoch 4/500\n",
      "512/512 [==============================] - 98s 190ms/step - loss: 0.3552 - acc: 0.8309 - val_loss: 0.3169 - val_acc: 0.8382\n",
      "Epoch 5/500\n",
      "512/512 [==============================] - 97s 189ms/step - loss: 0.3399 - acc: 0.8400 - val_loss: 0.2573 - val_acc: 0.8672\n",
      "Epoch 6/500\n",
      "512/512 [==============================] - 97s 189ms/step - loss: 0.3368 - acc: 0.8453 - val_loss: 0.3353 - val_acc: 0.8133\n",
      "Epoch 7/500\n",
      "512/512 [==============================] - 99s 194ms/step - loss: 0.3306 - acc: 0.8452 - val_loss: 0.3756 - val_acc: 0.8091\n",
      "Epoch 8/500\n",
      "512/512 [==============================] - 97s 190ms/step - loss: 0.3237 - acc: 0.8473 - val_loss: 0.3255 - val_acc: 0.8423\n",
      "Epoch 9/500\n",
      "512/512 [==============================] - 97s 189ms/step - loss: 0.3139 - acc: 0.8570 - val_loss: 0.3859 - val_acc: 0.8050\n",
      "Epoch 10/500\n",
      "512/512 [==============================] - 97s 189ms/step - loss: 0.3185 - acc: 0.8507 - val_loss: 0.2839 - val_acc: 0.8423\n",
      "Epoch 11/500\n",
      "512/512 [==============================] - 97s 189ms/step - loss: 0.3063 - acc: 0.8595 - val_loss: 0.3229 - val_acc: 0.8382\n",
      "Epoch 12/500\n",
      "512/512 [==============================] - 97s 189ms/step - loss: 0.3011 - acc: 0.8651 - val_loss: 0.2843 - val_acc: 0.8506\n",
      "Epoch 13/500\n",
      "512/512 [==============================] - 96s 188ms/step - loss: 0.3041 - acc: 0.8622 - val_loss: 0.2941 - val_acc: 0.8631\n",
      "Epoch 14/500\n",
      "512/512 [==============================] - 97s 189ms/step - loss: 0.2963 - acc: 0.8668 - val_loss: 0.2908 - val_acc: 0.8465\n",
      "Epoch 15/500\n",
      "512/512 [==============================] - 97s 189ms/step - loss: 0.2894 - acc: 0.8702 - val_loss: 0.2788 - val_acc: 0.8548\n",
      "Epoch 16/500\n",
      "512/512 [==============================] - 97s 190ms/step - loss: 0.2941 - acc: 0.8676 - val_loss: 0.3756 - val_acc: 0.8382\n",
      "Epoch 17/500\n",
      "512/512 [==============================] - 97s 189ms/step - loss: 0.2877 - acc: 0.8719 - val_loss: 0.2518 - val_acc: 0.8589\n",
      "Epoch 18/500\n",
      "512/512 [==============================] - 96s 188ms/step - loss: 0.2861 - acc: 0.8724 - val_loss: 0.2586 - val_acc: 0.8714\n",
      "Epoch 19/500\n",
      "512/512 [==============================] - 97s 189ms/step - loss: 0.2762 - acc: 0.8768 - val_loss: 0.2294 - val_acc: 0.9046\n",
      "Epoch 20/500\n",
      "512/512 [==============================] - 96s 188ms/step - loss: 0.2728 - acc: 0.8790 - val_loss: 0.2855 - val_acc: 0.8382\n",
      "Epoch 21/500\n",
      "512/512 [==============================] - 97s 189ms/step - loss: 0.2730 - acc: 0.8801 - val_loss: 0.2753 - val_acc: 0.8465\n",
      "Epoch 22/500\n",
      "512/512 [==============================] - 97s 189ms/step - loss: 0.2753 - acc: 0.8824 - val_loss: 0.2673 - val_acc: 0.8714\n",
      "Epoch 23/500\n",
      "512/512 [==============================] - 96s 188ms/step - loss: 0.2723 - acc: 0.8823 - val_loss: 0.2539 - val_acc: 0.8755\n",
      "Epoch 24/500\n",
      "512/512 [==============================] - 97s 189ms/step - loss: 0.2665 - acc: 0.8838 - val_loss: 0.2599 - val_acc: 0.8880\n",
      "Epoch 25/500\n",
      "512/512 [==============================] - 97s 189ms/step - loss: 0.2640 - acc: 0.8870 - val_loss: 0.2710 - val_acc: 0.8838\n",
      "Epoch 26/500\n",
      "512/512 [==============================] - 97s 189ms/step - loss: 0.2625 - acc: 0.8868 - val_loss: 0.2636 - val_acc: 0.8880\n",
      "Epoch 27/500\n",
      "512/512 [==============================] - 97s 189ms/step - loss: 0.2598 - acc: 0.8873 - val_loss: 0.2607 - val_acc: 0.8631\n",
      "Epoch 28/500\n",
      "512/512 [==============================] - 97s 189ms/step - loss: 0.2592 - acc: 0.8850 - val_loss: 0.2537 - val_acc: 0.8880\n",
      "Epoch 29/500\n",
      "512/512 [==============================] - 97s 190ms/step - loss: 0.2589 - acc: 0.8851 - val_loss: 0.2626 - val_acc: 0.8838\n",
      "Epoch 30/500\n",
      "512/512 [==============================] - 97s 189ms/step - loss: 0.2556 - acc: 0.8909 - val_loss: 0.3949 - val_acc: 0.8257\n",
      "Epoch 31/500\n",
      "512/512 [==============================] - 96s 188ms/step - loss: 0.2531 - acc: 0.8915 - val_loss: 0.2599 - val_acc: 0.9087\n",
      "Epoch 32/500\n",
      "512/512 [==============================] - 97s 190ms/step - loss: 0.2497 - acc: 0.8931 - val_loss: 0.2715 - val_acc: 0.8755\n",
      "Epoch 33/500\n",
      "512/512 [==============================] - 97s 189ms/step - loss: 0.2465 - acc: 0.8937 - val_loss: 0.2714 - val_acc: 0.8423\n",
      "Epoch 34/500\n",
      "512/512 [==============================] - 97s 190ms/step - loss: 0.2496 - acc: 0.8921 - val_loss: 0.2577 - val_acc: 0.8963\n",
      "Epoch 35/500\n",
      "512/512 [==============================] - 97s 190ms/step - loss: 0.2458 - acc: 0.8947 - val_loss: 0.2943 - val_acc: 0.8506\n",
      "Epoch 36/500\n",
      "512/512 [==============================] - 97s 190ms/step - loss: 0.2448 - acc: 0.8928 - val_loss: 0.3301 - val_acc: 0.8382\n",
      "Epoch 37/500\n",
      "512/512 [==============================] - 97s 190ms/step - loss: 0.2426 - acc: 0.8949 - val_loss: 0.3132 - val_acc: 0.8672\n",
      "Epoch 38/500\n",
      "512/512 [==============================] - 97s 190ms/step - loss: 0.2470 - acc: 0.8967 - val_loss: 0.3425 - val_acc: 0.8465\n",
      "Epoch 39/500\n",
      "512/512 [==============================] - 97s 190ms/step - loss: 0.2409 - acc: 0.8980 - val_loss: 0.2673 - val_acc: 0.8838\n",
      "Epoch 40/500\n",
      "512/512 [==============================] - 97s 190ms/step - loss: 0.2470 - acc: 0.8939 - val_loss: 0.3347 - val_acc: 0.8548 acc:\n",
      "Epoch 41/500\n",
      "512/512 [==============================] - 97s 190ms/step - loss: 0.2357 - acc: 0.8996 - val_loss: 0.2812 - val_acc: 0.9004\n",
      "Epoch 42/500\n",
      "512/512 [==============================] - 97s 190ms/step - loss: 0.2405 - acc: 0.8968 - val_loss: 0.2675 - val_acc: 0.8797\n",
      "Epoch 43/500\n",
      "512/512 [==============================] - 98s 191ms/step - loss: 0.2426 - acc: 0.8959 - val_loss: 0.2693 - val_acc: 0.8838\n",
      "Epoch 44/500\n",
      "512/512 [==============================] - 97s 190ms/step - loss: 0.2428 - acc: 0.8965 - val_loss: 0.2851 - val_acc: 0.8797\n",
      "Epoch 45/500\n",
      "512/512 [==============================] - 97s 190ms/step - loss: 0.2314 - acc: 0.9000 - val_loss: 0.2897 - val_acc: 0.8838\n",
      "Epoch 46/500\n",
      "512/512 [==============================] - 98s 190ms/step - loss: 0.2351 - acc: 0.8964 - val_loss: 0.3306 - val_acc: 0.8589\n",
      "Epoch 47/500\n",
      "512/512 [==============================] - 97s 190ms/step - loss: 0.2340 - acc: 0.8979 - val_loss: 0.3537 - val_acc: 0.8672\n",
      "Epoch 48/500\n",
      "512/512 [==============================] - 97s 190ms/step - loss: 0.2324 - acc: 0.8960 - val_loss: 0.2936 - val_acc: 0.8838\n",
      "Epoch 49/500\n",
      "512/512 [==============================] - 96s 188ms/step - loss: 0.2372 - acc: 0.8975 - val_loss: 0.3018 - val_acc: 0.8880\n",
      "Epoch 50/500\n",
      "512/512 [==============================] - 96s 188ms/step - loss: 0.2333 - acc: 0.9021 - val_loss: 0.2996 - val_acc: 0.8838\n",
      "Epoch 51/500\n",
      "512/512 [==============================] - 96s 188ms/step - loss: 0.2281 - acc: 0.9032 - val_loss: 0.2807 - val_acc: 0.8797\n",
      "Epoch 52/500\n",
      "512/512 [==============================] - 97s 189ms/step - loss: 0.2290 - acc: 0.9024 - val_loss: 0.3436 - val_acc: 0.8589\n",
      "Epoch 53/500\n",
      "512/512 [==============================] - 96s 188ms/step - loss: 0.2296 - acc: 0.9001 - val_loss: 0.3384 - val_acc: 0.8672\n",
      "Epoch 54/500\n",
      "512/512 [==============================] - 97s 189ms/step - loss: 0.2279 - acc: 0.9028 - val_loss: 0.3493 - val_acc: 0.8423\n",
      "Epoch 55/500\n",
      "512/512 [==============================] - 96s 188ms/step - loss: 0.2248 - acc: 0.9056 - val_loss: 0.3085 - val_acc: 0.8714\n",
      "Epoch 56/500\n",
      "512/512 [==============================] - 96s 188ms/step - loss: 0.2224 - acc: 0.9042 - val_loss: 0.3385 - val_acc: 0.8506\n",
      "Epoch 57/500\n",
      "512/512 [==============================] - 96s 188ms/step - loss: 0.2330 - acc: 0.9024 - val_loss: 0.3250 - val_acc: 0.8589\n",
      "Epoch 58/500\n",
      "512/512 [==============================] - 97s 190ms/step - loss: 0.2242 - acc: 0.9041 - val_loss: 0.3266 - val_acc: 0.8838\n",
      "Epoch 59/500\n",
      "512/512 [==============================] - 98s 192ms/step - loss: 0.2234 - acc: 0.9040 - val_loss: 0.3589 - val_acc: 0.8589\n",
      "Epoch 60/500\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.2183 - acc: 0.9091\n",
      "Epoch 00060: reducing learning rate to 0.00010000000474974513.\n",
      "512/512 [==============================] - 100s 196ms/step - loss: 0.2187 - acc: 0.9088 - val_loss: 0.3144 - val_acc: 0.8963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/500\n",
      "512/512 [==============================] - 98s 191ms/step - loss: 0.2142 - acc: 0.9092 - val_loss: 0.2938 - val_acc: 0.8838\n",
      "Epoch 62/500\n",
      "512/512 [==============================] - 97s 190ms/step - loss: 0.2115 - acc: 0.9087 - val_loss: 0.3138 - val_acc: 0.8921\n",
      "Epoch 63/500\n",
      "512/512 [==============================] - 97s 190ms/step - loss: 0.2140 - acc: 0.9083 - val_loss: 0.3295 - val_acc: 0.8880\n",
      "Epoch 64/500\n",
      "512/512 [==============================] - 97s 190ms/step - loss: 0.2126 - acc: 0.9117 - val_loss: 0.3116 - val_acc: 0.8838\n",
      "Epoch 65/500\n",
      "512/512 [==============================] - 97s 190ms/step - loss: 0.2158 - acc: 0.9085 - val_loss: 0.3438 - val_acc: 0.8838\n",
      "Epoch 66/500\n",
      "512/512 [==============================] - 97s 190ms/step - loss: 0.2094 - acc: 0.9128 - val_loss: 0.3387 - val_acc: 0.8755\n",
      "Epoch 67/500\n",
      "512/512 [==============================] - 97s 190ms/step - loss: 0.2128 - acc: 0.9099 - val_loss: 0.3401 - val_acc: 0.8714\n",
      "Epoch 68/500\n",
      "512/512 [==============================] - 97s 190ms/step - loss: 0.2132 - acc: 0.9104 - val_loss: 0.3182 - val_acc: 0.8797\n",
      "Epoch 69/500\n",
      "512/512 [==============================] - 97s 190ms/step - loss: 0.2091 - acc: 0.9105 - val_loss: 0.3214 - val_acc: 0.8838\n",
      "Epoch 00069: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18bdc5a25c0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "batch_size = 32\n",
    "#Lets define the image transormations that we want\n",
    "gen = ImageDataGenerator(horizontal_flip=True,\n",
    "                         vertical_flip=True,\n",
    "                         width_shift_range=0.1,\n",
    "                         height_shift_range=0.1,\n",
    "                         zoom_range=0.1,\n",
    "                         rotation_range=90)\n",
    "\n",
    "# Here is the function that merges our two generators\n",
    "def gen_flow_for_two_inputs(X1, X2, y):\n",
    "    genX1 = gen.flow(X1,y,  batch_size=batch_size,seed=666)\n",
    "    genX2 = gen.flow(X1,X2, batch_size=batch_size,seed=666)\n",
    "    while True:\n",
    "            X1i = genX1.next()\n",
    "            X2i = genX2.next()\n",
    "            #Assert arrays are equal - this was for peace of mind, but slows down training\n",
    "            #np.testing.assert_array_equal(X1i[0],X2i[0])\n",
    "            yield [X1i[0], X2i[1]], X1i[1]\n",
    "\n",
    "# Finally create generator\n",
    "gen_flow = gen_flow_for_two_inputs(X_train, X_angle_train, y_train)\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "#call backs\n",
    "weights_file = './model.h5'\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=50, verbose=1, min_delta=1e-4, mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=40, verbose=1, epsilon=1e-4, mode='min')\n",
    "model_chk = ModelCheckpoint(monitor='val_loss', filepath=weights_file, save_best_only=True, \n",
    "                            save_weights_only=True, mode='min')\n",
    "        \n",
    "callbacks = [earlystop, reduce_lr_loss, model_chk, TensorBoard(log_dir='./logs1')]\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.002), \n",
    "              loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#fit the model\n",
    "model.fit_generator(gen_flow, validation_data=([X_valid, X_angle_valid], y_valid), \n",
    "                    steps_per_epoch=np.ceil(12 * float(len(y_train)) / float(batch_size)), \n",
    "                    epochs=500, verbose=1, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitted: (1348, 199, 199, 2), (256, 199, 199, 2)\n",
      "splitted: (1348,), (256,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\preprocessing\\image.py:855: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (1348, 199, 199, 2) (2 channels).\n",
      "  ' (' + str(self.x.shape[channels_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      " - 227s - loss: 0.7768 - acc: 0.6080 - val_loss: 0.5501 - val_acc: 0.6562\n",
      "Epoch 2/1000\n",
      " - 223s - loss: 0.6647 - acc: 0.6359 - val_loss: 0.8509 - val_acc: 0.6094\n",
      "Epoch 3/1000\n",
      " - 222s - loss: 0.6292 - acc: 0.6624 - val_loss: 0.6694 - val_acc: 0.7266\n",
      "Epoch 4/1000\n",
      " - 222s - loss: 0.5671 - acc: 0.7160 - val_loss: 0.5135 - val_acc: 0.7695\n",
      "Epoch 5/1000\n",
      " - 223s - loss: 0.5097 - acc: 0.7713 - val_loss: 0.4972 - val_acc: 0.7852\n",
      "Epoch 6/1000\n",
      " - 223s - loss: 0.4549 - acc: 0.7917 - val_loss: 0.4711 - val_acc: 0.8008\n",
      "Epoch 7/1000\n",
      " - 222s - loss: 0.4221 - acc: 0.8051 - val_loss: 0.4220 - val_acc: 0.8125\n",
      "Epoch 8/1000\n",
      " - 222s - loss: 0.4232 - acc: 0.8059 - val_loss: 0.4748 - val_acc: 0.7852\n",
      "Epoch 9/1000\n",
      " - 223s - loss: 0.4080 - acc: 0.8081 - val_loss: 0.3783 - val_acc: 0.8281\n",
      "Epoch 10/1000\n",
      " - 222s - loss: 0.3845 - acc: 0.8217 - val_loss: 0.4678 - val_acc: 0.7930\n",
      "Epoch 11/1000\n",
      " - 222s - loss: 0.3867 - acc: 0.8217 - val_loss: 0.4340 - val_acc: 0.7969\n",
      "Epoch 12/1000\n",
      " - 222s - loss: 0.3755 - acc: 0.8276 - val_loss: 0.4191 - val_acc: 0.7969\n",
      "Epoch 13/1000\n",
      " - 223s - loss: 0.3663 - acc: 0.8284 - val_loss: 0.3671 - val_acc: 0.8320\n",
      "Epoch 14/1000\n",
      " - 222s - loss: 0.3621 - acc: 0.8308 - val_loss: 0.5488 - val_acc: 0.7578\n",
      "Epoch 15/1000\n",
      " - 222s - loss: 0.3624 - acc: 0.8255 - val_loss: 0.6174 - val_acc: 0.7539\n",
      "Epoch 16/1000\n",
      " - 222s - loss: 0.3696 - acc: 0.8270 - val_loss: 0.5350 - val_acc: 0.7852\n",
      "Epoch 17/1000\n",
      " - 222s - loss: 0.3503 - acc: 0.8421 - val_loss: 0.5253 - val_acc: 0.7773\n",
      "Epoch 18/1000\n",
      " - 222s - loss: 0.3463 - acc: 0.8367 - val_loss: 0.5434 - val_acc: 0.7578\n",
      "Epoch 19/1000\n",
      " - 222s - loss: 0.3579 - acc: 0.8379 - val_loss: 0.5331 - val_acc: 0.7812\n",
      "Epoch 20/1000\n",
      " - 222s - loss: 0.3602 - acc: 0.8369 - val_loss: 0.5499 - val_acc: 0.7578\n",
      "Epoch 21/1000\n",
      " - 222s - loss: 0.3499 - acc: 0.8412 - val_loss: 0.5659 - val_acc: 0.7812\n",
      "Epoch 22/1000\n",
      " - 223s - loss: 0.3453 - acc: 0.8397 - val_loss: 0.7128 - val_acc: 0.7227\n",
      "Epoch 23/1000\n",
      " - 222s - loss: 0.3458 - acc: 0.8433 - val_loss: 0.6116 - val_acc: 0.7227\n",
      "Epoch 24/1000\n",
      " - 222s - loss: 0.3458 - acc: 0.8405 - val_loss: 0.5186 - val_acc: 0.7891\n",
      "Epoch 25/1000\n",
      " - 222s - loss: 0.3372 - acc: 0.8428 - val_loss: 0.6632 - val_acc: 0.7227\n",
      "Epoch 26/1000\n",
      " - 223s - loss: 0.3287 - acc: 0.8504 - val_loss: 0.5706 - val_acc: 0.7695\n",
      "Epoch 27/1000\n",
      " - 222s - loss: 0.3316 - acc: 0.8453 - val_loss: 0.4970 - val_acc: 0.7969\n",
      "Epoch 28/1000\n",
      " - 222s - loss: 0.3234 - acc: 0.8505 - val_loss: 0.6877 - val_acc: 0.7188\n",
      "Epoch 29/1000\n",
      " - 222s - loss: 0.3308 - acc: 0.8424 - val_loss: 0.5324 - val_acc: 0.7695\n",
      "Epoch 30/1000\n",
      " - 222s - loss: 0.3364 - acc: 0.8461 - val_loss: 0.6112 - val_acc: 0.7695\n",
      "Epoch 31/1000\n",
      " - 223s - loss: 0.3373 - acc: 0.8473 - val_loss: 0.5197 - val_acc: 0.7773\n",
      "Epoch 32/1000\n",
      " - 222s - loss: 0.3232 - acc: 0.8493 - val_loss: 0.5646 - val_acc: 0.7617\n",
      "Epoch 33/1000\n",
      " - 222s - loss: 0.3215 - acc: 0.8501 - val_loss: 0.4819 - val_acc: 0.7891\n",
      "Epoch 34/1000\n",
      " - 222s - loss: 0.3326 - acc: 0.8487 - val_loss: 0.6395 - val_acc: 0.7070\n",
      "Epoch 35/1000\n",
      " - 223s - loss: 0.3255 - acc: 0.8526 - val_loss: 0.6381 - val_acc: 0.7578\n",
      "Epoch 36/1000\n",
      " - 222s - loss: 0.3294 - acc: 0.8499 - val_loss: 0.7182 - val_acc: 0.7188\n",
      "Epoch 37/1000\n",
      " - 222s - loss: 0.3232 - acc: 0.8547 - val_loss: 0.7364 - val_acc: 0.6992\n",
      "Epoch 38/1000\n",
      " - 222s - loss: 0.3227 - acc: 0.8491 - val_loss: 0.6209 - val_acc: 0.7500\n",
      "Epoch 39/1000\n",
      " - 223s - loss: 0.3238 - acc: 0.8518 - val_loss: 0.5176 - val_acc: 0.7852\n",
      "Epoch 40/1000\n",
      " - 222s - loss: 0.3140 - acc: 0.8567 - val_loss: 0.5834 - val_acc: 0.7773\n",
      "Epoch 41/1000\n",
      " - 222s - loss: 0.3120 - acc: 0.8568 - val_loss: 0.5804 - val_acc: 0.7695\n",
      "Epoch 42/1000\n",
      " - 224s - loss: 0.3205 - acc: 0.8511 - val_loss: 0.6467 - val_acc: 0.7383\n",
      "Epoch 43/1000\n",
      " - 223s - loss: 0.3073 - acc: 0.8547 - val_loss: 0.8042 - val_acc: 0.7188\n",
      "Epoch 44/1000\n",
      " - 223s - loss: 0.3116 - acc: 0.8588 - val_loss: 0.7450 - val_acc: 0.7148\n",
      "Epoch 45/1000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "#train, validataion split\n",
    "test_ratio = 0.159\n",
    "nr_runs = 5\n",
    "split_seed = 25\n",
    "epochs = 1000\n",
    "kf = StratifiedShuffleSplit(n_splits=nr_runs, test_size=test_ratio, train_size=None, random_state=split_seed)\n",
    "\n",
    "train_y = train['is_iceberg'].values\n",
    "split_indices = train_y.copy()\n",
    "\n",
    "train_X = rgb_train\n",
    "train_meta = train['inc_angle'].values\n",
    "\n",
    "#training, evaluation, test and make submission\n",
    "for r, (train_index, valid_index) in enumerate(kf.split(train, split_indices)):\n",
    "    tmp = dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "        \n",
    "    y1, y2 = train_y[train_index], train_y[valid_index]\n",
    "    x1, x2 = train_X[train_index], train_X[valid_index]\n",
    "    xm1, xm2 = train_meta[train_index], train_meta[valid_index]\n",
    "\n",
    "    print('splitted: {0}, {1}'.format(x1.shape, x2.shape), flush=True)\n",
    "    print('splitted: {0}, {1}'.format(y1.shape, y2.shape), flush=True)\n",
    "    ################################\n",
    "    if r > 0:\n",
    "        model.load_weights('model.h5')\n",
    "        \n",
    "    #optim = SGD(lr=0.005, momentum=0.0, decay=0.002, nesterov=True)\n",
    "    optim = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.002)\n",
    "        \n",
    "    model.compile(optimizer=optim, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    #call backs\n",
    "    earlystop = EarlyStopping(monitor='val_loss', patience=100, verbose=1, min_delta=1e-4, mode='min')\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=40, verbose=1, epsilon=1e-4, mode='min')\n",
    "    model_chk = ModelCheckpoint(monitor='val_loss', filepath='model.h5', save_best_only=True, \n",
    "                                save_weights_only=True, mode='min')\n",
    "        \n",
    "    callbacks = [earlystop, reduce_lr_loss, model_chk, TensorBoard(log_dir='./logs_cv')]\n",
    "        ##########\n",
    "       \n",
    "    model.fit_generator(generator=gen_flow_for_two_inputs(x1, xm1, y1),\n",
    "                        steps_per_epoch= np.ceil(12 * float(len(y1)) / float(batch_size)),\n",
    "                        epochs=epochs,\n",
    "                        verbose=2,\n",
    "                        callbacks=callbacks,\n",
    "                        validation_data=([x2, xm2], y2))\n",
    "\n",
    "     \n",
    "\n",
    "    model.load_weights('model.h5')\n",
    "            \n",
    "    p = model.predict([x2, xm2], batch_size=batch_size, verbose=1)\n",
    "    print('\\n\\nEvaluate loss in validation data: {}'.format(log_loss(y2, p)), flush=True)\n",
    "\n",
    "    p = model.predict([x1, xm1], batch_size=batch_size, verbose=1)\n",
    "    print('\\n\\nEvaluate loss in training data: {}'.format(log_loss(y1, p)), flush=True)\n",
    "           \n",
    "    print('\\nPredict...', flush=True)\n",
    "    ids = test['id'].values\n",
    "\n",
    "    #prediction\n",
    "    pred = model.predict([test_X_dup, test_meta], batch_size=batch_size, verbose=1)\n",
    "    pred = np.squeeze(pred, axis=-1)\n",
    "            \n",
    "    file = 'subm_{}_f{:03d}.csv'.format(tmp, nb_filters)\n",
    "    subm = pd.DataFrame({'id': ids, target: pred})\n",
    "    subm.to_csv('./submitions/{}'.format(file), index=False, float_format='%.6f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1363/1363 [==============================] - 3s 2ms/step\n",
      "[0.28756552283062448, 0.87160675042880775]\n",
      "241/241 [==============================] - 1s 2ms/step\n",
      "[0.22941713109303313, 0.9045643153526971]\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "from keras.models import load_model\n",
    "model.load_weights('model.h5')\n",
    "\n",
    "print(model.evaluate([X_train,X_angle_train], y_train))\n",
    "print(model.evaluate([X_valid,X_angle_valid], y_valid))\n",
    "\n",
    "test_predictions = model.predict([rgb_test, X_angle_test])\n",
    "\n",
    "tmp = dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "nb_filters = 2\n",
    "file = 'subm_{}_f{:03d}.csv'.format(tmp, nb_filters)\n",
    "\n",
    "# Create .csv\n",
    "pred_df = test[['id']].copy()\n",
    "pred_df['is_iceberg'] = test_predictions\n",
    "pred_df.to_csv('./submitions/{}'.format(file), index=False, float_format='%.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
