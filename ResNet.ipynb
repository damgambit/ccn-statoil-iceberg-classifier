{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D\n",
    "from keras.layers import BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "import pydot\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import keras\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "from resnets_utils import *\n",
    "from keras.initializers import glorot_uniform\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import datetime as dt\n",
    "import cv2\n",
    "data_dir = './' #Change to your directory here\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "K.set_learning_phase(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data loaded\n",
      "Test data loaded\n",
      "done\n",
      "RGBs Done\n",
      "X_train: (1203, 75, 75, 3)\n",
      "X_valid: (401, 75, 75, 3)\n"
     ]
    }
   ],
   "source": [
    "def load_data(data_dir):\n",
    "    train = pd.read_json(data_dir + 'train.json')\n",
    "    print(\"Train data loaded\")\n",
    "    test = pd.read_json(data_dir + 'test.json')\n",
    "    print(\"Test data loaded\")\n",
    "    #Fill 'na' angles with mode\n",
    "    train.inc_angle = train.inc_angle.replace('na', 0)\n",
    "    train.inc_angle = train.inc_angle.astype(float).fillna(0.0)\n",
    "    test.inc_angle = test.inc_angle.replace('na', 0)\n",
    "    test.inc_angle = test.inc_angle.astype(float).fillna(0.0)\n",
    "    return train, test\n",
    "\n",
    "train, test = load_data(data_dir)\n",
    "print(\"done\")\n",
    "\n",
    "X_angle_train = np.array(train.inc_angle)\n",
    "X_angle_test = np.array(test.inc_angle)\n",
    "\n",
    "\n",
    "def color_composite(data):\n",
    "    w,h = 75,75\n",
    "    rgb_arrays = []\n",
    "    for i, row in data.iterrows():\n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        band_3 = (band_1 + band_2)/2\n",
    "        band_4 = band_2 * row['inc_angle']\n",
    "\n",
    "\n",
    "        r = (band_1 + abs(band_1.min())) / np.max((band_1 + abs(band_1.min())))\n",
    "        g = (band_2 + abs(band_2.min())) / np.max((band_2 + abs(band_2.min())))\n",
    "        b = (band_3 + abs(band_3.min())) / np.max((band_3 + abs(band_3.min())))\n",
    "        a = (band_3 + abs(band_3.min())) / np.max((band_3 + abs(band_3.min())))\n",
    "\n",
    "        \n",
    "        rgba = np.dstack((r, g, b))\n",
    "\n",
    "        #rgba = cv2.resize(rgba, (w,h)).astype(np.float32)\n",
    "        \n",
    "        rgb_arrays.append(rgba)\n",
    "    return np.array(rgb_arrays)\n",
    "\n",
    "rgb_train = color_composite(train)\n",
    "rgb_test = color_composite(test)\n",
    "print(\"RGBs Done\")\n",
    "\n",
    "y_train = np.array(train['is_iceberg'])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, X_angle_train, X_angle_valid, y_train, y_valid = train_test_split(rgb_train,\n",
    "                                    X_angle_train, y_train, random_state=420, test_size=0.25)\n",
    "\n",
    "print(\"X_train:\",X_train.shape)\n",
    "print(\"X_valid:\",X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identity_block(X, f, filters, stage, block):\n",
    "    \"\"\"\n",
    "    Implementation of the identity block as defined in Figure 3\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    # Save the input value.\n",
    "    X_shortcut = X\n",
    "    \n",
    "    # First component of main path\n",
    "    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(name = bn_name_base + '2a')(X)\n",
    "    X = Activation('elu')(X)\n",
    "    \n",
    "    \n",
    "    # Second component of main path \n",
    "    X = Conv2D(F2, kernel_size=(f,f), strides=(1,1), padding='same', name= conv_name_base+'2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(name = bn_name_base + '2b')(X)\n",
    "    X = Activation('elu')(X)\n",
    "\n",
    "    # Third component of main path \n",
    "    X = Conv2D(F3, kernel_size=(1,1), strides=(1,1), padding='valid', name= conv_name_base+'2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(name = bn_name_base + '2c')(X)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation \n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('elu')(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolutional_block(X, f, filters, stage, block, s = 2):\n",
    "    \"\"\"\n",
    "    Implementation of the convolutional block as defined in Figure 4\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "    s -- Integer, specifying the stride to be used\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "\n",
    "\n",
    "    # First component of main path \n",
    "    X = Conv2D(F1, (1, 1), strides = (s,s), name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(name = bn_name_base + '2a')(X)\n",
    "    X = Activation('elu')(X)\n",
    "\n",
    "\n",
    "    # Second component of main path\n",
    "    X = Conv2D(F2, kernel_size=(f,f), strides=(1,1), padding='same', name=conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(name=bn_name_base+'2b')(X)\n",
    "    X = Activation('elu')(X)\n",
    "\n",
    "    # Third component of main path\n",
    "    X = Conv2D(F3, (1, 1), strides = (1,1), name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(name=bn_name_base+'2c')(X)\n",
    "\n",
    "    X_shortcut = Conv2D(F3, (1, 1), strides = (s,s), name = conv_name_base + '1', kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + '1')(X_shortcut)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (â‰ˆ2 lines)\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('elu')(X)\n",
    "\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ResNet50(input_shape = (75, 75, 3), classes = 1):\n",
    "    \"\"\"\n",
    "    Implementation of the popular ResNet50 the following architecture:\n",
    "    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
    "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    input_1 = Input(shape=(75, 75, 3), name=\"X_1\")\n",
    "    input_2 = Input(shape=[1], name=\"angle\")\n",
    "\n",
    "    \n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((0, 0))(input_1)\n",
    "    \n",
    "    # Stage 1\n",
    "    X = Conv2D(16, (3, 3), strides = (2, 2), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(name = 'bn_conv1')(X)\n",
    "    X = Activation('elu')(X)\n",
    "   # X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    # Stage 2\n",
    "    X = convolutional_block(X, f = 3, filters = [16, 16, 64], stage = 2, block='a', s = 1)\n",
    "    X = identity_block(X, 3, [16, 16, 64], stage=2, block='b')\n",
    "    X = identity_block(X, 3, [16, 16, 64], stage=2, block='c')\n",
    "    X = identity_block(X, 3, [16, 16, 64], stage=2, block='d')\n",
    "\n",
    "\n",
    "\n",
    "    # Stage 3 \n",
    "    X = convolutional_block(X, f = 3, filters = [32, 32, 128], stage = 3, block='a', s = 2)\n",
    "    X = identity_block(X, 3, [32, 32, 128], stage=3, block='b')\n",
    "    X = identity_block(X, 3, [32,32, 128], stage=3, block='c')\n",
    "    X = identity_block(X, 3, [32,32, 128], stage=3, block='d')\n",
    "    X = identity_block(X, 3, [32, 32, 128], stage=3, block='e')\n",
    "    X = identity_block(X, 3, [32,32, 128], stage=3, block='f')\n",
    "\n",
    "    # Stage 4\n",
    "    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 4, block='a', s = 2)\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=4, block='b')\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=4, block='c')\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=4, block='d')\n",
    "\n",
    "    \n",
    "    # AVGPOOL\n",
    "    X = AveragePooling2D(name='avg_pool')(X)\n",
    "\n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Concatenate()([X, BatchNormalization()(input_2)])\n",
    "    X = Dense(classes, activation='sigmoid', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs = [input_1, input_2], outputs = X, name='ResNet50')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "X_1 (InputLayer)                (None, 75, 75, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 75, 75, 3)    0           X_1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 37, 37, 16)   448         zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 37, 37, 16)   64          conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 37, 37, 16)   0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 37, 37, 16)   272         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 37, 37, 16)   64          res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 37, 37, 16)   0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 37, 37, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 37, 37, 16)   64          res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 37, 37, 16)   0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 37, 37, 64)   1088        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 37, 37, 64)   1088        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 37, 37, 64)   256         res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 37, 37, 64)   256         res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 37, 37, 64)   0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 37, 37, 64)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 37, 37, 16)   1040        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 37, 37, 16)   64          res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 37, 37, 16)   0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 37, 37, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 37, 37, 16)   64          res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 37, 37, 16)   0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 37, 37, 64)   1088        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 37, 37, 64)   256         res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 37, 37, 64)   0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 37, 37, 64)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 37, 37, 16)   1040        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 37, 37, 16)   64          res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 37, 37, 16)   0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 37, 37, 16)   2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 37, 37, 16)   64          res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 37, 37, 16)   0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 37, 37, 64)   1088        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 37, 37, 64)   256         res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 37, 37, 64)   0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 37, 37, 64)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2d_branch2a (Conv2D)         (None, 37, 37, 16)   1040        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2d_branch2a (BatchNormalizati (None, 37, 37, 16)   64          res2d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 37, 37, 16)   0           bn2d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2d_branch2b (Conv2D)         (None, 37, 37, 16)   2320        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2d_branch2b (BatchNormalizati (None, 37, 37, 16)   64          res2d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 37, 37, 16)   0           bn2d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2d_branch2c (Conv2D)         (None, 37, 37, 64)   1088        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2d_branch2c (BatchNormalizati (None, 37, 37, 64)   256         res2d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 37, 37, 64)   0           bn2d_branch2c[0][0]              \n",
      "                                                                 activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 37, 37, 64)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 19, 19, 32)   2080        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 19, 19, 32)   128         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 19, 19, 32)   0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 19, 19, 32)   9248        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 19, 19, 32)   128         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 19, 19, 32)   0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 19, 19, 128)  4224        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 19, 19, 128)  8320        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 19, 19, 128)  512         res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 19, 19, 128)  512         res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 19, 19, 128)  0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 19, 19, 128)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 19, 19, 32)   4128        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 19, 19, 32)   128         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 19, 19, 32)   0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 19, 19, 32)   9248        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 19, 19, 32)   128         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 19, 19, 32)   0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 19, 19, 128)  4224        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 19, 19, 128)  512         res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 19, 19, 128)  0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 19, 19, 128)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 19, 19, 32)   4128        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 19, 19, 32)   128         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 19, 19, 32)   0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 19, 19, 32)   9248        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 19, 19, 32)   128         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 19, 19, 32)   0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 19, 19, 128)  4224        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 19, 19, 128)  512         res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 19, 19, 128)  0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 19, 19, 128)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 19, 19, 32)   4128        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 19, 19, 32)   128         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 19, 19, 32)   0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 19, 19, 32)   9248        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 19, 19, 32)   128         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 19, 19, 32)   0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 19, 19, 128)  4224        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 19, 19, 128)  512         res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 19, 19, 128)  0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 19, 19, 128)  0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3e_branch2a (Conv2D)         (None, 19, 19, 32)   4128        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3e_branch2a (BatchNormalizati (None, 19, 19, 32)   128         res3e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 19, 19, 32)   0           bn3e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3e_branch2b (Conv2D)         (None, 19, 19, 32)   9248        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3e_branch2b (BatchNormalizati (None, 19, 19, 32)   128         res3e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 19, 19, 32)   0           bn3e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3e_branch2c (Conv2D)         (None, 19, 19, 128)  4224        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3e_branch2c (BatchNormalizati (None, 19, 19, 128)  512         res3e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 19, 19, 128)  0           bn3e_branch2c[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 19, 19, 128)  0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3f_branch2a (Conv2D)         (None, 19, 19, 32)   4128        activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3f_branch2a (BatchNormalizati (None, 19, 19, 32)   128         res3f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 19, 19, 32)   0           bn3f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3f_branch2b (Conv2D)         (None, 19, 19, 32)   9248        activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3f_branch2b (BatchNormalizati (None, 19, 19, 32)   128         res3f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 19, 19, 32)   0           bn3f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3f_branch2c (Conv2D)         (None, 19, 19, 128)  4224        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3f_branch2c (BatchNormalizati (None, 19, 19, 128)  512         res3f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 19, 19, 128)  0           bn3f_branch2c[0][0]              \n",
      "                                                                 activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 19, 19, 128)  0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 10, 10, 64)   8256        activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 10, 10, 64)   256         res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 10, 10, 64)   0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 10, 10, 64)   36928       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 10, 10, 64)   256         res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 10, 10, 64)   0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 10, 10, 256)  16640       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 10, 10, 256)  33024       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 10, 10, 256)  1024        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 10, 10, 256)  1024        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 10, 10, 256)  0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 10, 10, 256)  0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 10, 10, 64)   16448       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 10, 10, 64)   256         res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 10, 10, 64)   0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 10, 10, 64)   36928       activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 10, 10, 64)   256         res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 10, 10, 64)   0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 10, 10, 256)  16640       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 10, 10, 256)  1024        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 10, 10, 256)  0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 10, 10, 256)  0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 10, 10, 64)   16448       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 10, 10, 64)   256         res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 10, 10, 64)   0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 10, 10, 64)   36928       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 10, 10, 64)   256         res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 10, 10, 64)   0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 10, 10, 256)  16640       activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 10, 10, 256)  1024        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 10, 10, 256)  0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 10, 10, 256)  0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 10, 10, 64)   16448       activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 10, 10, 64)   256         res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 10, 10, 64)   0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 10, 10, 64)   36928       activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 10, 10, 64)   256         res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 10, 10, 64)   0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 10, 10, 256)  16640       activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 10, 10, 256)  1024        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 10, 10, 256)  0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 10, 10, 256)  0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (AveragePooling2D)     (None, 5, 5, 256)    0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "angle (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 6400)         0           avg_pool[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1)            4           angle[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 6401)         0           flatten_1[0][0]                  \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fc1 (Dense)                     (None, 1)            6402        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 455,878\n",
      "Trainable params: 448,804\n",
      "Non-trainable params: 7,074\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "model = ResNet50()\n",
    "\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "264/264 [==============================] - 68s 258ms/step - loss: 0.3406 - acc: 0.6593 - val_loss: 0.5276 - val_acc: 0.4724\n",
      "Epoch 2/500\n",
      "264/264 [==============================] - 59s 223ms/step - loss: 0.3269 - acc: 0.6728 - val_loss: 0.3206 - val_acc: 0.6799\n",
      "Epoch 3/500\n",
      "264/264 [==============================] - 58s 222ms/step - loss: 0.3247 - acc: 0.6752 - val_loss: 0.3070 - val_acc: 0.6942\n",
      "Epoch 4/500\n",
      "264/264 [==============================] - 58s 220ms/step - loss: 0.3169 - acc: 0.6835 - val_loss: 0.2785 - val_acc: 0.7230\n",
      "Epoch 5/500\n",
      "264/264 [==============================] - 59s 224ms/step - loss: 0.2936 - acc: 0.7064 - val_loss: 0.4996 - val_acc: 0.5000\n",
      "Epoch 6/500\n",
      "264/264 [==============================] - 59s 225ms/step - loss: 0.2793 - acc: 0.7205 - val_loss: 0.2470 - val_acc: 0.7530\n",
      "Epoch 7/500\n",
      "264/264 [==============================] - 58s 220ms/step - loss: 0.2441 - acc: 0.7557 - val_loss: 0.2376 - val_acc: 0.7656\n",
      "Epoch 8/500\n",
      "264/264 [==============================] - 58s 221ms/step - loss: 0.2344 - acc: 0.7659 - val_loss: 0.2082 - val_acc: 0.7926\n",
      "Epoch 9/500\n",
      "264/264 [==============================] - 58s 219ms/step - loss: 0.2355 - acc: 0.7644 - val_loss: 0.1883 - val_acc: 0.8118\n",
      "Epoch 10/500\n",
      "264/264 [==============================] - 58s 220ms/step - loss: 0.2208 - acc: 0.7790 - val_loss: 0.2073 - val_acc: 0.7926\n",
      "Epoch 11/500\n",
      "264/264 [==============================] - 58s 220ms/step - loss: 0.2241 - acc: 0.7759 - val_loss: 0.1715 - val_acc: 0.8261\n",
      "Epoch 12/500\n",
      "264/264 [==============================] - 58s 219ms/step - loss: 0.2249 - acc: 0.7746 - val_loss: 0.2127 - val_acc: 0.7878\n",
      "Epoch 13/500\n",
      "264/264 [==============================] - 58s 219ms/step - loss: 0.2248 - acc: 0.7760 - val_loss: 0.2166 - val_acc: 0.7806\n",
      "Epoch 14/500\n",
      "264/264 [==============================] - 58s 219ms/step - loss: 0.2169 - acc: 0.7831 - val_loss: 0.1794 - val_acc: 0.8213\n",
      "Epoch 15/500\n",
      "264/264 [==============================] - 58s 219ms/step - loss: 0.2160 - acc: 0.7841 - val_loss: 0.2081 - val_acc: 0.7914\n",
      "Epoch 16/500\n",
      "264/264 [==============================] - 58s 219ms/step - loss: 0.2075 - acc: 0.7929 - val_loss: 0.2099 - val_acc: 0.7914\n",
      "Epoch 17/500\n",
      "264/264 [==============================] - 58s 219ms/step - loss: 0.2092 - acc: 0.7907 - val_loss: 0.1931 - val_acc: 0.8070\n",
      "Epoch 18/500\n",
      "264/264 [==============================] - 59s 222ms/step - loss: 0.2065 - acc: 0.7939 - val_loss: 0.1781 - val_acc: 0.8225\n",
      "Epoch 19/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.2076 - acc: 0.7920 - val_loss: 0.1799 - val_acc: 0.8189\n",
      "Epoch 20/500\n",
      "264/264 [==============================] - 70s 266ms/step - loss: 0.2052 - acc: 0.7947 - val_loss: 0.1834 - val_acc: 0.8177\n",
      "Epoch 21/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.2105 - acc: 0.7897 - val_loss: 0.1780 - val_acc: 0.8225\n",
      "Epoch 22/500\n",
      "264/264 [==============================] - 70s 264ms/step - loss: 0.2029 - acc: 0.7973 - val_loss: 0.1803 - val_acc: 0.8189\n",
      "Epoch 23/500\n",
      "264/264 [==============================] - 70s 266ms/step - loss: 0.2029 - acc: 0.7973 - val_loss: 0.1690 - val_acc: 0.8309\n",
      "Epoch 24/500\n",
      "264/264 [==============================] - 70s 264ms/step - loss: 0.2017 - acc: 0.7985 - val_loss: 0.1838 - val_acc: 0.8165\n",
      "Epoch 25/500\n",
      "264/264 [==============================] - 70s 264ms/step - loss: 0.1996 - acc: 0.7999 - val_loss: 0.1805 - val_acc: 0.8177\n",
      "Epoch 26/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.1987 - acc: 0.8010 - val_loss: 0.1705 - val_acc: 0.8303\n",
      "Epoch 27/500\n",
      "264/264 [==============================] - 68s 257ms/step - loss: 0.1980 - acc: 0.8021 - val_loss: 0.1743 - val_acc: 0.8237\n",
      "Epoch 28/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.1973 - acc: 0.8026 - val_loss: 0.1614 - val_acc: 0.8393\n",
      "Epoch 29/500\n",
      "264/264 [==============================] - 70s 264ms/step - loss: 0.1965 - acc: 0.8037 - val_loss: 0.1748 - val_acc: 0.8249\n",
      "Epoch 30/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.1915 - acc: 0.8088 - val_loss: 0.1724 - val_acc: 0.8273\n",
      "Epoch 31/500\n",
      "264/264 [==============================] - 70s 264ms/step - loss: 0.1941 - acc: 0.8063 - val_loss: 0.1678 - val_acc: 0.8321\n",
      "Epoch 32/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.2009 - acc: 0.7992 - val_loss: 0.1705 - val_acc: 0.8297\n",
      "Epoch 33/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.1892 - acc: 0.8112 - val_loss: 0.1661 - val_acc: 0.8333\n",
      "Epoch 34/500\n",
      "264/264 [==============================] - 70s 263ms/step - loss: 0.1923 - acc: 0.8081 - val_loss: 0.1657 - val_acc: 0.8357\n",
      "Epoch 35/500\n",
      "264/264 [==============================] - 70s 264ms/step - loss: 0.1910 - acc: 0.8098 - val_loss: 0.1660 - val_acc: 0.8333\n",
      "Epoch 36/500\n",
      "264/264 [==============================] - 70s 267ms/step - loss: 0.1902 - acc: 0.8100 - val_loss: 0.1592 - val_acc: 0.8405\n",
      "Epoch 37/500\n",
      "264/264 [==============================] - 70s 264ms/step - loss: 0.1903 - acc: 0.8092 - val_loss: 0.1670 - val_acc: 0.8333\n",
      "Epoch 38/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.1903 - acc: 0.8094 - val_loss: 0.1577 - val_acc: 0.8417\n",
      "Epoch 39/500\n",
      "264/264 [==============================] - 70s 267ms/step - loss: 0.1913 - acc: 0.8089 - val_loss: 0.1614 - val_acc: 0.8376\n",
      "Epoch 40/500\n",
      "264/264 [==============================] - 70s 266ms/step - loss: 0.1881 - acc: 0.8124 - val_loss: 0.1521 - val_acc: 0.8501\n",
      "Epoch 41/500\n",
      "264/264 [==============================] - 70s 264ms/step - loss: 0.1932 - acc: 0.8070 - val_loss: 0.1657 - val_acc: 0.8357\n",
      "Epoch 42/500\n",
      "264/264 [==============================] - 70s 266ms/step - loss: 0.1916 - acc: 0.8080 - val_loss: 0.1692 - val_acc: 0.8297\n",
      "Epoch 43/500\n",
      "264/264 [==============================] - 70s 267ms/step - loss: 0.1854 - acc: 0.8153 - val_loss: 0.1732 - val_acc: 0.8273\n",
      "Epoch 44/500\n",
      "264/264 [==============================] - 67s 255ms/step - loss: 0.1815 - acc: 0.8186 - val_loss: 0.1725 - val_acc: 0.8273\n",
      "Epoch 45/500\n",
      "264/264 [==============================] - 68s 259ms/step - loss: 0.1830 - acc: 0.8167 - val_loss: 0.1666 - val_acc: 0.8333\n",
      "Epoch 46/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.1782 - acc: 0.8228 - val_loss: 0.1862 - val_acc: 0.8141\n",
      "Epoch 47/500\n",
      "264/264 [==============================] - 70s 266ms/step - loss: 0.1835 - acc: 0.8168 - val_loss: 0.1881 - val_acc: 0.8118\n",
      "Epoch 48/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.1849 - acc: 0.8152 - val_loss: 0.1629 - val_acc: 0.8381\n",
      "Epoch 49/500\n",
      "264/264 [==============================] - 70s 264ms/step - loss: 0.1793 - acc: 0.8205 - val_loss: 0.1620 - val_acc: 0.8369\n",
      "Epoch 50/500\n",
      "264/264 [==============================] - 70s 263ms/step - loss: 0.1823 - acc: 0.8183 - val_loss: 0.1619 - val_acc: 0.8381\n",
      "Epoch 51/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.1789 - acc: 0.8219 - val_loss: 0.1683 - val_acc: 0.8309\n",
      "Epoch 52/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.1910 - acc: 0.8088 - val_loss: 0.1642 - val_acc: 0.8364\n",
      "Epoch 53/500\n",
      "264/264 [==============================] - 68s 257ms/step - loss: 0.1818 - acc: 0.8182 - val_loss: 0.1612 - val_acc: 0.8381\n",
      "Epoch 54/500\n",
      "264/264 [==============================] - 70s 266ms/step - loss: 0.1821 - acc: 0.8183 - val_loss: 0.1377 - val_acc: 0.8621\n",
      "Epoch 55/500\n",
      "264/264 [==============================] - 70s 266ms/step - loss: 0.1793 - acc: 0.8216 - val_loss: 0.1511 - val_acc: 0.8489\n",
      "Epoch 56/500\n",
      "264/264 [==============================] - 70s 264ms/step - loss: 0.1827 - acc: 0.8172 - val_loss: 0.1553 - val_acc: 0.8441\n",
      "Epoch 57/500\n",
      "264/264 [==============================] - 70s 264ms/step - loss: 0.1788 - acc: 0.8211 - val_loss: 0.1571 - val_acc: 0.8417\n",
      "Epoch 61/500\n",
      "264/264 [==============================] - 69s 263ms/step - loss: 0.1805 - acc: 0.8189 - val_loss: 0.1479 - val_acc: 0.8525\n",
      "Epoch 62/500\n",
      "264/264 [==============================] - 68s 257ms/step - loss: 0.1797 - acc: 0.8210 - val_loss: 0.1493 - val_acc: 0.8537\n",
      "Epoch 63/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.1739 - acc: 0.8260 - val_loss: 0.1593 - val_acc: 0.8405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.1733 - acc: 0.8263 - val_loss: 0.1724 - val_acc: 0.8273\n",
      "Epoch 65/500\n",
      "264/264 [==============================] - 70s 264ms/step - loss: 0.1745 - acc: 0.8255 - val_loss: 0.1539 - val_acc: 0.8462\n",
      "Epoch 66/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.1781 - acc: 0.8225 - val_loss: 0.1491 - val_acc: 0.8525\n",
      "Epoch 67/500\n",
      "264/264 [==============================] - 70s 264ms/step - loss: 0.1791 - acc: 0.8202 - val_loss: 0.1628 - val_acc: 0.8381\n",
      "Epoch 68/500\n",
      "264/264 [==============================] - 70s 264ms/step - loss: 0.1804 - acc: 0.8200 - val_loss: 0.1433 - val_acc: 0.8537\n",
      "Epoch 69/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.1752 - acc: 0.8253 - val_loss: 0.1427 - val_acc: 0.8549\n",
      "Epoch 70/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.1827 - acc: 0.8174 - val_loss: 0.1636 - val_acc: 0.8369\n",
      "Epoch 71/500\n",
      "264/264 [==============================] - 70s 263ms/step - loss: 0.1747 - acc: 0.8254 - val_loss: 0.1550 - val_acc: 0.8429\n",
      "Epoch 72/500\n",
      "264/264 [==============================] - 70s 263ms/step - loss: 0.1824 - acc: 0.8174 - val_loss: 0.1739 - val_acc: 0.8249\n",
      "Epoch 73/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.1720 - acc: 0.8286 - val_loss: 0.1419 - val_acc: 0.8585\n",
      "Epoch 74/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.1728 - acc: 0.8270 - val_loss: 0.1534 - val_acc: 0.8453\n",
      "Epoch 75/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.1739 - acc: 0.8264 - val_loss: 0.1564 - val_acc: 0.8453\n",
      "Epoch 76/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.1750 - acc: 0.8249 - val_loss: 0.1668 - val_acc: 0.8321\n",
      "Epoch 77/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.1724 - acc: 0.8283 - val_loss: 0.1514 - val_acc: 0.8513\n",
      "Epoch 78/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.1745 - acc: 0.8265 - val_loss: 0.1536 - val_acc: 0.8462\n",
      "Epoch 79/500\n",
      "264/264 [==============================] - 67s 254ms/step - loss: 0.1720 - acc: 0.8282 - val_loss: 0.1589 - val_acc: 0.8417\n",
      "Epoch 80/500\n",
      "264/264 [==============================] - 70s 264ms/step - loss: 0.1743 - acc: 0.8256 - val_loss: 0.1604 - val_acc: 0.8393\n",
      "Epoch 81/500\n",
      "264/264 [==============================] - 70s 266ms/step - loss: 0.1753 - acc: 0.8245 - val_loss: 0.1420 - val_acc: 0.8573\n",
      "Epoch 82/500\n",
      "264/264 [==============================] - 70s 266ms/step - loss: 0.1786 - acc: 0.8209 - val_loss: 0.1535 - val_acc: 0.8465\n",
      "Epoch 83/500\n",
      "264/264 [==============================] - 70s 264ms/step - loss: 0.1683 - acc: 0.8324 - val_loss: 0.1447 - val_acc: 0.8573\n",
      "Epoch 84/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.1746 - acc: 0.8261 - val_loss: 0.1498 - val_acc: 0.8489\n",
      "Epoch 85/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.1748 - acc: 0.8256 - val_loss: 0.1564 - val_acc: 0.8429\n",
      "Epoch 86/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.1722 - acc: 0.8278 - val_loss: 0.1546 - val_acc: 0.8441\n",
      "Epoch 87/500\n",
      "264/264 [==============================] - 70s 264ms/step - loss: 0.1699 - acc: 0.8301 - val_loss: 0.1503 - val_acc: 0.8489\n",
      "Epoch 88/500\n",
      "264/264 [==============================] - 68s 258ms/step - loss: 0.1712 - acc: 0.8291 - val_loss: 0.1552 - val_acc: 0.8441\n",
      "Epoch 89/500\n",
      "264/264 [==============================] - 70s 264ms/step - loss: 0.1709 - acc: 0.8295 - val_loss: 0.1463 - val_acc: 0.8537\n",
      "Epoch 90/500\n",
      "264/264 [==============================] - 70s 263ms/step - loss: 0.1704 - acc: 0.8294 - val_loss: 0.1591 - val_acc: 0.8393\n",
      "Epoch 91/500\n",
      "264/264 [==============================] - 69s 263ms/step - loss: 0.1718 - acc: 0.8281 - val_loss: 0.1521 - val_acc: 0.8474\n",
      "Epoch 92/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.1695 - acc: 0.8306 - val_loss: 0.1561 - val_acc: 0.8441\n",
      "Epoch 93/500\n",
      "264/264 [==============================] - 70s 264ms/step - loss: 0.1660 - acc: 0.8336 - val_loss: 0.1517 - val_acc: 0.8465\n",
      "Epoch 94/500\n",
      "264/264 [==============================] - 68s 257ms/step - loss: 0.1652 - acc: 0.8351 - val_loss: 0.1315 - val_acc: 0.8693\n",
      "Epoch 98/500\n",
      "264/264 [==============================] - 69s 263ms/step - loss: 0.1669 - acc: 0.8329 - val_loss: 0.1435 - val_acc: 0.8573\n",
      "Epoch 99/500\n",
      "264/264 [==============================] - 70s 263ms/step - loss: 0.1711 - acc: 0.8292 - val_loss: 0.1380 - val_acc: 0.8621\n",
      "Epoch 100/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.1685 - acc: 0.8315 - val_loss: 0.1424 - val_acc: 0.8561\n",
      "Epoch 101/500\n",
      "264/264 [==============================] - 69s 263ms/step - loss: 0.1651 - acc: 0.8351 - val_loss: 0.1577 - val_acc: 0.8441\n",
      "Epoch 102/500\n",
      "264/264 [==============================] - 69s 263ms/step - loss: 0.1662 - acc: 0.8343 - val_loss: 0.1497 - val_acc: 0.8501\n",
      "Epoch 103/500\n",
      "264/264 [==============================] - 69s 263ms/step - loss: 0.1699 - acc: 0.8315 - val_loss: 0.1507 - val_acc: 0.8501\n",
      "Epoch 104/500\n",
      "264/264 [==============================] - 70s 263ms/step - loss: 0.1686 - acc: 0.8317 - val_loss: 0.1498 - val_acc: 0.8486\n",
      "Epoch 105/500\n",
      "264/264 [==============================] - 69s 262ms/step - loss: 0.1713 - acc: 0.8285 - val_loss: 0.1523 - val_acc: 0.8465\n",
      "Epoch 106/500\n",
      "264/264 [==============================] - 68s 257ms/step - loss: 0.1650 - acc: 0.8352 - val_loss: 0.1526 - val_acc: 0.8489\n",
      "Epoch 107/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.1616 - acc: 0.8385 - val_loss: 0.1407 - val_acc: 0.8609\n",
      "Epoch 108/500\n",
      "264/264 [==============================] - 70s 265ms/step - loss: 0.1680 - acc: 0.8319 - val_loss: 0.1574 - val_acc: 0.8429\n",
      "Epoch 109/500\n",
      "264/264 [==============================] - 71s 268ms/step - loss: 0.1656 - acc: 0.8347 - val_loss: 0.1494 - val_acc: 0.8501\n",
      "Epoch 110/500\n",
      "264/264 [==============================] - 71s 268ms/step - loss: 0.1641 - acc: 0.8364 - val_loss: 0.1459 - val_acc: 0.8537\n",
      "Epoch 111/500\n",
      "264/264 [==============================] - 70s 266ms/step - loss: 0.1705 - acc: 0.8299 - val_loss: 0.1539 - val_acc: 0.8441\n",
      "Epoch 112/500\n",
      "264/264 [==============================] - 70s 267ms/step - loss: 0.1660 - acc: 0.8344 - val_loss: 0.1593 - val_acc: 0.8393\n",
      "Epoch 113/500\n",
      "264/264 [==============================] - 71s 267ms/step - loss: 0.1699 - acc: 0.8305 - val_loss: 0.1456 - val_acc: 0.8549\n",
      "Epoch 114/500\n",
      "264/264 [==============================] - 67s 254ms/step - loss: 0.1660 - acc: 0.8338 - val_loss: 0.1355 - val_acc: 0.8657\n",
      "Epoch 115/500\n",
      "264/264 [==============================] - 71s 267ms/step - loss: 0.1649 - acc: 0.8354 - val_loss: 0.1431 - val_acc: 0.8561\n",
      "Epoch 116/500\n",
      "264/264 [==============================] - 70s 266ms/step - loss: 0.1685 - acc: 0.8322 - val_loss: 0.1472 - val_acc: 0.8525\n",
      "Epoch 117/500\n",
      "264/264 [==============================] - 70s 267ms/step - loss: 0.1636 - acc: 0.8368 - val_loss: 0.1397 - val_acc: 0.8608\n",
      "Epoch 118/500\n",
      "264/264 [==============================] - 71s 267ms/step - loss: 0.1654 - acc: 0.8349 - val_loss: 0.1395 - val_acc: 0.8597\n",
      "Epoch 119/500\n",
      "264/264 [==============================] - 71s 268ms/step - loss: 0.1643 - acc: 0.8355 - val_loss: 0.1482 - val_acc: 0.8537\n",
      "Epoch 120/500\n",
      "264/264 [==============================] - 68s 259ms/step - loss: 0.1640 - acc: 0.8365 - val_loss: 0.1494 - val_acc: 0.8501\n",
      "Epoch 124/500\n",
      "264/264 [==============================] - 71s 268ms/step - loss: 0.1706 - acc: 0.8292 - val_loss: 0.1420 - val_acc: 0.8585\n",
      "Epoch 125/500\n",
      "264/264 [==============================] - 71s 269ms/step - loss: 0.1670 - acc: 0.8335 - val_loss: 0.1427 - val_acc: 0.8573\n",
      "Epoch 126/500\n",
      "264/264 [==============================] - 71s 268ms/step - loss: 0.1635 - acc: 0.8364 - val_loss: 0.1354 - val_acc: 0.8645\n",
      "Epoch 127/500\n",
      "264/264 [==============================] - 71s 268ms/step - loss: 0.1676 - acc: 0.8328 - val_loss: 0.1381 - val_acc: 0.8621\n",
      "Epoch 128/500\n",
      "264/264 [==============================] - 71s 268ms/step - loss: 0.1682 - acc: 0.8322 - val_loss: 0.1493 - val_acc: 0.8510\n",
      "Epoch 131/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264/264 [==============================] - 71s 269ms/step - loss: 0.1641 - acc: 0.8364 - val_loss: 0.1442 - val_acc: 0.8573\n",
      "Epoch 132/500\n",
      "264/264 [==============================] - 69s 261ms/step - loss: 0.1650 - acc: 0.8346 - val_loss: 0.1465 - val_acc: 0.8537\n",
      "Epoch 133/500\n",
      "264/264 [==============================] - 70s 267ms/step - loss: 0.1682 - acc: 0.8328 - val_loss: 0.1489 - val_acc: 0.8525\n",
      "Epoch 134/500\n",
      "264/264 [==============================] - 71s 268ms/step - loss: 0.1696 - acc: 0.8304 - val_loss: 0.1493 - val_acc: 0.8525\n",
      "Epoch 135/500\n",
      "264/264 [==============================] - 71s 269ms/step - loss: 0.1644 - acc: 0.8366 - val_loss: 0.1401 - val_acc: 0.8585\n",
      "Epoch 136/500\n",
      "264/264 [==============================] - 71s 270ms/step - loss: 0.1634 - acc: 0.8368 - val_loss: 0.1522 - val_acc: 0.8477\n",
      "Epoch 137/500\n",
      "264/264 [==============================] - 71s 268ms/step - loss: 0.1629 - acc: 0.8373 - val_loss: 0.1437 - val_acc: 0.8561\n",
      "Epoch 138/500\n",
      "263/264 [============================>.] - ETA: 0s - loss: 0.1643 - acc: 0.8368\n",
      "Epoch 00138: reducing learning rate to 1.0000000474974514e-05.\n",
      "264/264 [==============================] - 71s 270ms/step - loss: 0.1646 - acc: 0.8365 - val_loss: 0.1558 - val_acc: 0.8465\n",
      "Epoch 139/500\n",
      "264/264 [==============================] - 71s 270ms/step - loss: 0.1666 - acc: 0.8344 - val_loss: 0.1543 - val_acc: 0.8453\n",
      "Epoch 140/500\n",
      "264/264 [==============================] - 68s 259ms/step - loss: 0.1654 - acc: 0.8348 - val_loss: 0.1626 - val_acc: 0.8369\n",
      "Epoch 141/500\n",
      "264/264 [==============================] - 71s 268ms/step - loss: 0.1686 - acc: 0.8312 - val_loss: 0.1445 - val_acc: 0.8561\n",
      "Epoch 142/500\n",
      "264/264 [==============================] - 71s 269ms/step - loss: 0.1694 - acc: 0.8305 - val_loss: 0.1449 - val_acc: 0.8561\n",
      "Epoch 143/500\n",
      "264/264 [==============================] - 71s 269ms/step - loss: 0.1689 - acc: 0.8304 - val_loss: 0.1453 - val_acc: 0.8559\n",
      "Epoch 144/500\n",
      "264/264 [==============================] - 71s 270ms/step - loss: 0.1660 - acc: 0.8336 - val_loss: 0.1619 - val_acc: 0.8369\n",
      "Epoch 145/500\n",
      "264/264 [==============================] - 71s 270ms/step - loss: 0.1697 - acc: 0.8304 - val_loss: 0.1343 - val_acc: 0.8669\n",
      "Epoch 146/500\n",
      "264/264 [==============================] - 71s 268ms/step - loss: 0.1635 - acc: 0.8370 - val_loss: 0.1487 - val_acc: 0.8489\n",
      "Epoch 147/500\n",
      "264/264 [==============================] - 71s 270ms/step - loss: 0.1645 - acc: 0.8359 - val_loss: 0.1431 - val_acc: 0.8561\n",
      "Epoch 00147: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19f3b1d7978>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "#Lets define the image transormations that we want\n",
    "gen = ImageDataGenerator(horizontal_flip=True,\n",
    "                         vertical_flip=True,\n",
    "                         width_shift_range=0.1,\n",
    "                         height_shift_range=0.1,\n",
    "                         zoom_range=0.1,\n",
    "                         rotation_range=90)\n",
    "\n",
    "# Here is the function that merges our two generators\n",
    "def gen_flow_for_two_inputs(X1, X2, y):\n",
    "    genX1 = gen.flow(X1,y,  batch_size=batch_size,seed=12)\n",
    "    genX2 = gen.flow(X1,X2, batch_size=batch_size,seed=12)\n",
    "    while True:\n",
    "            X1i = genX1.next()\n",
    "            X2i = genX2.next()\n",
    "            #Assert arrays are equal - this was for peace of mind, but slows down training\n",
    "            #np.testing.assert_array_equal(X1i[0],X2i[0])\n",
    "            yield [X1i[0], X2i[1]], X1i[1]\n",
    "\n",
    "# Finally create generator\n",
    "gen_flow = gen_flow_for_two_inputs(X_train, X_angle_train, y_train)\n",
    "gen_flow_valid = gen_flow_for_two_inputs(X_valid, X_angle_valid, y_valid)\n",
    "\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "#call backs\n",
    "weights_file = './model.h5'\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=50, verbose=1, min_delta=1e-4, mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=40, verbose=1, epsilon=1e-4, mode='min')\n",
    "model_chk = ModelCheckpoint(monitor='val_loss', filepath=weights_file, save_best_only=True, \n",
    "                            save_weights_only=True, mode='min')\n",
    "        \n",
    "callbacks = [earlystop, reduce_lr_loss, model_chk, TensorBoard(log_dir='./logs1')]\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.002), \n",
    "              loss='mae', metrics=['accuracy'])\n",
    "\n",
    "#fit the model\n",
    "model.fit_generator(gen_flow, validation_data=gen_flow_valid, \n",
    "                    validation_steps=np.ceil((7 * float(len(y_train)) / float(batch_size))/10), \n",
    "                    steps_per_epoch=np.ceil(7 * float(len(y_train)) / float(batch_size)), \n",
    "                    epochs=500, verbose=1, callbacks=callbacks)\n",
    "#model.fit([X_train, X_angle_train],y_train, validation_data=([X_valid, X_angle_valid], y_valid),  \n",
    " #                   epochs=500, verbose=1, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "from keras.models import load_model\n",
    "model.load_weights('model.h5')\n",
    "\n",
    "print(model.evaluate([X_train,X_angle_train], y_train))\n",
    "print(model.evaluate([X_valid,X_angle_valid], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'keras_learning_phase' with dtype bool\n\t [[Node: keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\t [[Node: bn_conv1/moments/sufficient_statistics/Gather/_3969 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_627_bn_conv1/moments/sufficient_statistics/Gather\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'keras_learning_phase', defined at:\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-49-028ce6584f01>\", line 2, in <module>\n    K.clear_session()\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 86, in clear_session\n    phase = tf.placeholder(dtype='bool', name='keras_learning_phase')\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1507, in placeholder\n    name=name)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 1997, in _placeholder\n    name=name)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'keras_learning_phase' with dtype bool\n\t [[Node: keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\t [[Node: bn_conv1/moments/sufficient_statistics/Gather/_3969 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_627_bn_conv1/moments/sufficient_statistics/Gather\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'keras_learning_phase' with dtype bool\n\t [[Node: keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\t [[Node: bn_conv1/moments/sufficient_statistics/Gather/_3969 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_627_bn_conv1/moments/sufficient_statistics/Gather\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-272ee3b59c81>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrgb_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_angle_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%Y-%m-%d-%H-%M\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnb_filters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'subm_{}_f{:03d}.csv'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_filters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1746\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1747\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[1;32m-> 1748\u001b[1;33m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[0;32m   1749\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1750\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[1;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1297\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1299\u001b[1;33m                 \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1300\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1301\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2332\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2333\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1052\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'keras_learning_phase' with dtype bool\n\t [[Node: keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\t [[Node: bn_conv1/moments/sufficient_statistics/Gather/_3969 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_627_bn_conv1/moments/sufficient_statistics/Gather\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'keras_learning_phase', defined at:\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-49-028ce6584f01>\", line 2, in <module>\n    K.clear_session()\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 86, in clear_session\n    phase = tf.placeholder(dtype='bool', name='keras_learning_phase')\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1507, in placeholder\n    name=name)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 1997, in _placeholder\n    name=name)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'keras_learning_phase' with dtype bool\n\t [[Node: keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\t [[Node: bn_conv1/moments/sufficient_statistics/Gather/_3969 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_627_bn_conv1/moments/sufficient_statistics/Gather\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "test_predictions = model.predict([rgb_test, X_angle_test])\n",
    "\n",
    "tmp = dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "nb_filters = 2\n",
    "file = 'subm_{}_f{:03d}.csv'.format(tmp, nb_filters)\n",
    "\n",
    "# Create .csv\n",
    "pred_df = test[['id']].copy()\n",
    "pred_df['is_iceberg'] = test_predictions\n",
    "pred_df.to_csv('./submitions/{}'.format(file), index=False, float_format='%.4f')\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
