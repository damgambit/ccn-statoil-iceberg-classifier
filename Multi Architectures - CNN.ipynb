{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data loaded\n",
      "Test data loaded\n",
      "done\n",
      "RGBs Done\n",
      "X_train: (1203, 75, 75, 3)\n",
      "X_valid: (401, 75, 75, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "data_dir = './' #Change to your directory here\n",
    "\n",
    "def load_data(data_dir):\n",
    "    train = pd.read_json(data_dir + 'train.json')\n",
    "    print(\"Train data loaded\")\n",
    "    test = pd.read_json(data_dir + 'test.json')\n",
    "    print(\"Test data loaded\")\n",
    "    #Fill 'na' angles with mode\n",
    "    train.inc_angle = train.inc_angle.replace('na', 0)\n",
    "    train.inc_angle = train.inc_angle.astype(float).fillna(0.0)\n",
    "    test.inc_angle = test.inc_angle.replace('na', 0)\n",
    "    test.inc_angle = test.inc_angle.astype(float).fillna(0.0)\n",
    "    return train, test\n",
    "\n",
    "train, test = load_data(data_dir)\n",
    "print(\"done\")\n",
    "\n",
    "def color_composite(data):\n",
    "    w,h = 75,75\n",
    "    rgb_arrays = []\n",
    "    for i, row in data.iterrows():\n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        band_3 = band_1 / band_2\n",
    "\n",
    "        r = (band_1 + abs(band_1.min())) / np.max((band_1 + abs(band_1.min())))\n",
    "        g = (band_2 + abs(band_2.min())) / np.max((band_2 + abs(band_2.min())))\n",
    "        b = (band_3 + abs(band_3.min())) / np.max((band_3 + abs(band_3.min())))\n",
    "\n",
    "        rgb = np.dstack((r, g, b))\n",
    "        #Add in to resize for resnet50 use 197 x 197\n",
    "        rgb = cv2.resize(rgb, (w,h)).astype(np.float32)\n",
    "        rgb_arrays.append(rgb)\n",
    "    return np.array(rgb_arrays)\n",
    "\n",
    "rgb_train = color_composite(train)\n",
    "rgb_test = color_composite(test)\n",
    "print(\"RGBs Done\")\n",
    "\n",
    "y_train = np.array(train['is_iceberg'])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(rgb_train, y_train, random_state=420, train_size=0.75)\n",
    "\n",
    "print(\"X_train:\",X_train.shape)\n",
    "print(\"X_valid:\",X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_22 (Conv2D)           (None, 73, 73, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 73, 73, 32)        128       \n",
      "_________________________________________________________________\n",
      "p_re_lu_5 (PReLU)            (None, 73, 73, 32)        170528    \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 73, 73, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 34, 34, 64)        256       \n",
      "_________________________________________________________________\n",
      "p_re_lu_6 (PReLU)            (None, 34, 34, 64)        73984     \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 34, 34, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 15, 15, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 15, 15, 128)       512       \n",
      "_________________________________________________________________\n",
      "p_re_lu_7 (PReLU)            (None, 15, 15, 128)       28800     \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 15, 15, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               1605888   \n",
      "_________________________________________________________________\n",
      "p_re_lu_8 (PReLU)            (None, 256)               256       \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "p_re_lu_9 (PReLU)            (None, 128)               128       \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,006,753\n",
      "Trainable params: 2,006,305\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/500\n",
      "510/512 [============================>.] - ETA: 0s - loss: 0.5577 - acc: 0.7408Epoch 00001: val_loss improved from inf to 0.37506, saving model to model.h5\n",
      "512/512 [==============================] - 27s 52ms/step - loss: 0.5574 - acc: 0.7408 - val_loss: 0.3751 - val_acc: 0.7955\n",
      "Epoch 2/500\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.3963 - acc: 0.8148Epoch 00002: val_loss improved from 0.37506 to 0.29760, saving model to model.h5\n",
      "512/512 [==============================] - 21s 41ms/step - loss: 0.3970 - acc: 0.8145 - val_loss: 0.2976 - val_acc: 0.8728\n",
      "Epoch 3/500\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.3662 - acc: 0.8305Epoch 00003: val_loss did not improve\n",
      "512/512 [==============================] - 21s 41ms/step - loss: 0.3662 - acc: 0.8303 - val_loss: 0.3598 - val_acc: 0.8254\n",
      "Epoch 4/500\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.3390 - acc: 0.8460Epoch 00004: val_loss improved from 0.29760 to 0.27931, saving model to model.h5\n",
      "512/512 [==============================] - 21s 41ms/step - loss: 0.3392 - acc: 0.8459 - val_loss: 0.2793 - val_acc: 0.8778\n",
      "Epoch 5/500\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.3252 - acc: 0.8540Epoch 00005: val_loss did not improve\n",
      "512/512 [==============================] - 21s 41ms/step - loss: 0.3252 - acc: 0.8540 - val_loss: 0.2857 - val_acc: 0.8554\n",
      "Epoch 6/500\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.3124 - acc: 0.8587Epoch 00006: val_loss did not improve\n",
      "512/512 [==============================] - 21s 41ms/step - loss: 0.3122 - acc: 0.8586 - val_loss: 0.3025 - val_acc: 0.8504\n",
      "Epoch 7/500\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.2944 - acc: 0.8655Epoch 00007: val_loss did not improve\n",
      "512/512 [==============================] - 21s 41ms/step - loss: 0.2942 - acc: 0.8656 - val_loss: 0.2808 - val_acc: 0.8703\n",
      "Epoch 8/500\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.2799 - acc: 0.8765Epoch 00008: val_loss improved from 0.27931 to 0.26668, saving model to model.h5\n",
      "512/512 [==============================] - 21s 41ms/step - loss: 0.2796 - acc: 0.8767 - val_loss: 0.2667 - val_acc: 0.8579\n",
      "Epoch 9/500\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.2698 - acc: 0.8766Epoch 00009: val_loss did not improve\n",
      "512/512 [==============================] - 21s 41ms/step - loss: 0.2697 - acc: 0.8766 - val_loss: 0.3510 - val_acc: 0.8130\n",
      "Epoch 10/500\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.2600 - acc: 0.8906Epoch 00010: val_loss improved from 0.26668 to 0.25938, saving model to model.h5\n",
      "512/512 [==============================] - 21s 41ms/step - loss: 0.2597 - acc: 0.8907 - val_loss: 0.2594 - val_acc: 0.8803\n",
      "Epoch 11/500\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.2441 - acc: 0.8923Epoch 00011: val_loss did not improve\n",
      "512/512 [==============================] - 21s 41ms/step - loss: 0.2441 - acc: 0.8924 - val_loss: 0.2805 - val_acc: 0.8603\n",
      "Epoch 12/500\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.2462 - acc: 0.8973Epoch 00012: val_loss did not improve\n",
      "512/512 [==============================] - 21s 41ms/step - loss: 0.2460 - acc: 0.8974 - val_loss: 0.2741 - val_acc: 0.8853\n",
      "Epoch 13/500\n",
      "510/512 [============================>.] - ETA: 0s - loss: 0.2271 - acc: 0.9033Epoch 00013: val_loss did not improve\n",
      "512/512 [==============================] - 21s 42ms/step - loss: 0.2273 - acc: 0.9033 - val_loss: 0.2723 - val_acc: 0.8878\n",
      "Epoch 14/500\n",
      "510/512 [============================>.] - ETA: 0s - loss: 0.2248 - acc: 0.9050Epoch 00014: val_loss did not improve\n",
      "512/512 [==============================] - 21s 42ms/step - loss: 0.2254 - acc: 0.9049 - val_loss: 0.2685 - val_acc: 0.8878\n",
      "Epoch 15/500\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.2085 - acc: 0.9132Epoch 00015: val_loss did not improve\n",
      "512/512 [==============================] - 21s 41ms/step - loss: 0.2086 - acc: 0.9132 - val_loss: 0.2655 - val_acc: 0.8778\n",
      "Epoch 16/500\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.2002 - acc: 0.9178Epoch 00016: val_loss improved from 0.25938 to 0.23585, saving model to model.h5\n",
      "512/512 [==============================] - 21s 41ms/step - loss: 0.2004 - acc: 0.9177 - val_loss: 0.2358 - val_acc: 0.9052\n",
      "Epoch 17/500\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.1984 - acc: 0.9185Epoch 00017: val_loss did not improve\n",
      "512/512 [==============================] - 21s 41ms/step - loss: 0.1983 - acc: 0.9186 - val_loss: 0.2554 - val_acc: 0.8928\n",
      "Epoch 18/500\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.1865 - acc: 0.9197Epoch 00018: val_loss did not improve\n",
      "512/512 [==============================] - 22s 43ms/step - loss: 0.1866 - acc: 0.9197 - val_loss: 0.2960 - val_acc: 0.8803\n",
      "Epoch 19/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511/512 [============================>.] - ETA: 0s - loss: 0.1780 - acc: 0.9258Epoch 00019: val_loss did not improve\n",
      "512/512 [==============================] - 22s 43ms/step - loss: 0.1780 - acc: 0.9259 - val_loss: 0.3828 - val_acc: 0.8853\n",
      "Epoch 20/500\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9274Epoch 00020: val_loss did not improve\n",
      "512/512 [==============================] - 21s 41ms/step - loss: 0.1759 - acc: 0.9275 - val_loss: 0.3249 - val_acc: 0.8653\n",
      "Epoch 21/500\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.1725 - acc: 0.9314Epoch 00021: val_loss did not improve\n",
      "512/512 [==============================] - 21s 41ms/step - loss: 0.1724 - acc: 0.9315 - val_loss: 0.4156 - val_acc: 0.8579\n",
      "Epoch 22/500\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.1661 - acc: 0.9307Epoch 00022: val_loss did not improve\n",
      "512/512 [==============================] - 21s 41ms/step - loss: 0.1659 - acc: 0.9308 - val_loss: 0.3101 - val_acc: 0.8778\n",
      "Epoch 23/500\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.1530 - acc: 0.9380Epoch 00023: val_loss did not improve\n",
      "512/512 [==============================] - 21s 41ms/step - loss: 0.1528 - acc: 0.9381 - val_loss: 0.6140 - val_acc: 0.8005\n",
      "Epoch 24/500\n",
      "422/512 [=======================>......] - ETA: 3s - loss: 0.1490 - acc: 0.9393"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-90b3a808a08c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     97\u001b[0m model.fit_generator(gen_flow, validation_data=(X_valid, y_valid), \n\u001b[0;32m     98\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     verbose=1, callbacks=[early_stopping_callback, checkpoint_callback])\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1154\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1156\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2075\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2076\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2077\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2078\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2079\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1795\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1796\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1797\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1798\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1799\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2332\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2333\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.layers import GlobalMaxPooling2D, Dense, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPool2D\n",
    "from keras.layers import LeakyReLU, PReLU\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "import keras\n",
    "\n",
    "#Create the model\n",
    "#model = simple_cnn()\n",
    "#base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(197,197,3))\n",
    "#x = base_model.output\n",
    "#x = GlobalMaxPooling2D()(x)\n",
    "#x = Dense(1024, activation='relu')(x)\n",
    "#predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "#model = Model(inputs=base_model.input, outputs=predictions)\n",
    "#for layer in base_model.layers:\n",
    " #   layer.trainable = False\n",
    "\n",
    "def CNN(input_shape=(75, 75, 3)):\n",
    "    \n",
    "    def fully_connected(model, n_units, dropout_rate=0):\n",
    "        model.add(Dense(n_units, kernel_initializer='he_uniform'))\n",
    "        model.add(PReLU())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        return model\n",
    "    \n",
    "    def post_conv(model, dropout_rate=0):\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(PReLU())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        return model\n",
    "\n",
    "    def convolutional_layer(model, filters, kernel_size, padding = 'valid', input_shape=None, dropout_rate=0):\n",
    "        if input_shape is None:\n",
    "            model.add(Conv2D(filters, kernel_size=kernel_size, padding=padding, kernel_initializer='he_uniform'))\n",
    "        else:\n",
    "            model.add(Conv2D(filters, kernel_size=kernel_size, padding=padding, \n",
    "                             kernel_initializer='he_uniform', input_shape=input_shape))\n",
    "        return post_conv(model, dropout_rate)\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model = convolutional_layer(model, 32, kernel_size=(3,3), dropout_rate=0.2, input_shape=input_shape)\n",
    "    model = convolutional_layer(model, 64, kernel_size=(3,3), dropout_rate=0.2)\n",
    "    model = convolutional_layer(model, 128, kernel_size=(3,3), dropout_rate=0.2)  \n",
    "    #model = convolutional_layer(model, 256, kernel_size=(3,3), dropout_rate=0)    \n",
    "\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model = fully_connected(model, 256, dropout_rate=0.3)\n",
    "    model = fully_connected(model, 128, dropout_rate=0.3)\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "\n",
    "model = CNN()\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "batch_size = 32\n",
    "#Lets define the image transormations that we want\n",
    "gen = ImageDataGenerator(horizontal_flip=True,\n",
    "                         vertical_flip=True,\n",
    "                         width_shift_range=0.1,\n",
    "                         height_shift_range=0.1,\n",
    "                         zoom_range=0.1,\n",
    "                         rotation_range=40)\n",
    "\n",
    "# Here is the function that merges our two generators\n",
    "# We use the exact same generator with the same random seed for both the y and angle arrays\n",
    "def gen_flow_for_one_input(X1, y):\n",
    "    genX1 = gen.flow(X1, y, batch_size=batch_size, seed=420)\n",
    "    while True:\n",
    "        X1i = genX1.next()\n",
    "        yield X1i[0], X1i[1]\n",
    "\n",
    "#Finally create out generator\n",
    "gen_flow = gen_flow_for_one_input(X_train, y_train)\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "#call backs\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=100, verbose=1, min_delta=1e-4, mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=40, verbose=1, epsilon=1e-4, mode='min')\n",
    "model_chk = ModelCheckpoint(monitor='val_loss', filepath=weights_file, save_best_only=True, save_weights_only=True, mode='min')\n",
    "        \n",
    "callbacks = [earlystop, reduce_lr_loss, model_chk, TensorBoard(log_dir='../logs')]\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.002), \n",
    "              loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#fit the model\n",
    "model.fit_generator(gen_flow, validation_data=(X_valid, y_valid), \n",
    "    steps_per_epoch=2**14/batch_size, epochs=500, \n",
    "    verbose=1, callbacks=[early_stopping_callback, checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1203/1203 [==============================] - 1s 1ms/step\n",
      "[0.13342510869824084, 0.94846217798770516]\n",
      "401/401 [==============================] - 0s 517us/step\n",
      "[0.23584743628180829, 0.90523690802795331]\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "from keras.models import load_model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "print(model.evaluate(X_train, y_train))\n",
    "print(model.evaluate(X_valid, y_valid))\n",
    "\n",
    "test_predictions = model.predict(rgb_test)\n",
    "\n",
    "# Create .csv\n",
    "pred_df = test[['id']].copy()\n",
    "pred_df['is_iceberg'] = test_predictions\n",
    "pred_df.to_csv('model.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
