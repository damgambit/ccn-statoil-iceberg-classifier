{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import datetime as dt\n",
    "import cv2\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data loaded\n",
      "Test data loaded\n",
      "done\n",
      "RGBs Done\n"
     ]
    }
   ],
   "source": [
    "data_dir = './' #Change to your directory here\n",
    "\n",
    "def load_data(data_dir):\n",
    "    train = pd.read_json(data_dir + 'train.json')\n",
    "    print(\"Train data loaded\")\n",
    "    test = pd.read_json(data_dir + 'test.json')\n",
    "    print(\"Test data loaded\")\n",
    "    #Fill 'na' angles with mode\n",
    "    train.inc_angle = train.inc_angle.replace('na', 0)\n",
    "    train.inc_angle = train.inc_angle.astype(float).fillna(0.0)\n",
    "    test.inc_angle = test.inc_angle.replace('na', 0)\n",
    "    test.inc_angle = test.inc_angle.astype(float).fillna(0.0)\n",
    "    return train, test\n",
    "\n",
    "train, test = load_data(data_dir)\n",
    "print(\"done\")\n",
    "\n",
    "X_angle_train = np.array(train.inc_angle)\n",
    "X_angle_test = np.array(test.inc_angle)\n",
    "\n",
    "\n",
    "def color_composite(data):\n",
    "    w,h = 75,75\n",
    "    rgb_arrays = []\n",
    "    for i, row in data.iterrows():\n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        band_3 = (band_1 + band_2)/2\n",
    "        band_4 = band_2 * row['inc_angle']\n",
    "\n",
    "\n",
    "        r = (band_1 + abs(band_1.min())) / np.max((band_1 + abs(band_1.min())))\n",
    "        g = (band_2 + abs(band_2.min())) / np.max((band_2 + abs(band_2.min())))\n",
    "        b = (band_3 + abs(band_3.min())) / np.max((band_3 + abs(band_3.min())))\n",
    "        a = (band_3 + abs(band_3.min())) / np.max((band_3 + abs(band_3.min())))\n",
    "\n",
    "        \n",
    "        rgba = np.dstack((r, g, b))\n",
    "\n",
    "        #rgba = cv2.resize(rgba, (w,h)).astype(np.float32)\n",
    "        \n",
    "        rgb_arrays.append(rgba)\n",
    "    return np.array(rgb_arrays)\n",
    "\n",
    "rgb_train = color_composite(train)\n",
    "rgb_test = color_composite(test)\n",
    "print(\"RGBs Done\")\n",
    "\n",
    "y_train = np.array(train['is_iceberg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "def conv_block(x, nf=8, k=3, s=1, nb=2, p_act='elu'):\n",
    "    \n",
    "    for i in range(nb):\n",
    "        x = Conv2D(filters=nf, kernel_size=(k, k), strides=(s, s),  \n",
    "                   padding='same', kernel_initializer='he_uniform')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = keras.layers.LeakyReLU()(x)\n",
    "        \n",
    "    return x\n",
    "\n",
    "def res_block(x, nf=64, k=3, s=1, nb=1):\n",
    "    \n",
    "    for i in range(nb):\n",
    "        x = Conv2D(filters=nf, kernel_size=(k, k), strides=(s, s),  \n",
    "                   padding='same', kernel_initializer='he_uniform')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = keras.layers.LeakyReLU()(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        nf*=2\n",
    "        x = Conv2D(filters=nf, kernel_size=(k, k), strides=(s, s),  \n",
    "                   padding='same', kernel_initializer='he_uniform')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = keras.layers.LeakyReLU()(x)\n",
    "        \n",
    "    return x\n",
    "\n",
    "def dense_block(x, h=32, d=0.5, m=0., p_act='elu'):\n",
    "    return Dropout(d) (BatchNormalization(momentum=m) (Dense(h, activation=p_act)(x)))\n",
    "\n",
    "\n",
    "def bn_pooling(x, k=2, s=2, m=0): \n",
    "    return MaxPooling2D((k, k), strides=(s, s))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_simple_model_var1():\n",
    "    \n",
    "    bn_model = 0.99\n",
    "    p_activation = \"elu\"\n",
    "    input_1 = Input(shape=(75, 75, 3), name=\"X_1\")\n",
    "    input_2 = Input(shape=[1], name=\"angle\")\n",
    "    \n",
    "    img_1 = Conv2D(16, kernel_size = (5,5), activation=p_activation) ((BatchNormalization(momentum=bn_model))(input_1))\n",
    "    img_1 = Conv2D(16, kernel_size = (5,5), activation=p_activation) (img_1)\n",
    "    img_1 = MaxPooling2D((2,2)) (img_1)\n",
    "    img_1 = Dropout(0.1)(img_1)\n",
    "    \n",
    "    img_1 = Conv2D(32, kernel_size = (3,3), activation=p_activation) (img_1)\n",
    "    img_1 = Conv2D(32, kernel_size = (3,3), activation=p_activation) (img_1)\n",
    "    img_1 = MaxPooling2D((2,2)) (img_1)\n",
    "    img_1 = Dropout(0.1)(img_1)\n",
    "    \n",
    "    img_1 = Conv2D(64, kernel_size = (3,3), activation=p_activation) (img_1)\n",
    "    img_1 = Conv2D(64, kernel_size = (3,3), activation=p_activation) (img_1)\n",
    "    img_1 = MaxPooling2D((2,2)) (img_1)\n",
    "    img_1 = Dropout(0.1)(img_1)\n",
    "    \n",
    "    img_1 = Conv2D(128, kernel_size = (3,3), activation=p_activation) (img_1)\n",
    "    img_1 = MaxPooling2D((2,2)) (img_1)\n",
    "    img_1 = Dropout(0.1)(img_1)\n",
    "    img_1 = GlobalMaxPooling2D() (img_1)\n",
    "    \n",
    "    \n",
    "    img_2 = Conv2D(256, kernel_size = (3,3), activation=p_activation) ((BatchNormalization(momentum=bn_model))(input_1))\n",
    "    img_2 = MaxPooling2D((2,2)) (img_2)\n",
    "    img_2 = Dropout(0.1)(img_2)\n",
    "    img_2 = GlobalMaxPooling2D() (img_2)\n",
    "    \n",
    "    img_concat =  (Concatenate()([img_1, img_2, BatchNormalization(momentum=bn_model)(input_2)]))\n",
    "    \n",
    "    dense_ayer = Dropout(0.4) (BatchNormalization(momentum=bn_model) ( Dense(1024, activation=p_activation)(img_concat) ))\n",
    "    dense_ayer = Dropout(0.4) (BatchNormalization(momentum=bn_model) ( Dense(1024, activation=p_activation)(dense_ayer) ))\n",
    "    output = Dense(1, activation=\"sigmoid\")(dense_ayer)\n",
    "    \n",
    "    model = Model([input_1,input_2],  output)\n",
    "    optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def get_simple_model():\n",
    "    bn_model = 0.99\n",
    "    p_activation = \"elu\"\n",
    "    input_1 = Input(shape=(75, 75, 2), name=\"X_1\")\n",
    "    input_2 = Input(shape=[1], name=\"angle\")\n",
    "    \n",
    "    img_1 = Conv2D(16, kernel_size = (3,3), activation=p_activation) ((BatchNormalization(momentum=bn_model))(input_1))\n",
    "    img_1 = Conv2D(16, kernel_size = (3,3), activation=p_activation) (img_1)\n",
    "    img_1 = MaxPooling2D((2,2)) (img_1)\n",
    "    img_1 = Dropout(0.1)(img_1)\n",
    "    \n",
    "    img_1 = Conv2D(32, kernel_size = (3,3), activation=p_activation) (img_1)\n",
    "    img_1 = Conv2D(32, kernel_size = (3,3), activation=p_activation) (img_1)\n",
    "    img_1 = MaxPooling2D((2,2)) (img_1)\n",
    "    img_1 = Dropout(0.1)(img_1)\n",
    "    \n",
    "    img_1 = Conv2D(64, kernel_size = (3,3), activation=p_activation) (img_1)\n",
    "    img_1 = Conv2D(64, kernel_size = (3,3), activation=p_activation) (img_1)\n",
    "    img_1 = MaxPooling2D((2,2)) (img_1)\n",
    "    img_1 = Dropout(0.1)(img_1)\n",
    "    \n",
    "    img_1 = Conv2D(128, kernel_size = (3,3), activation=p_activation) (img_1)\n",
    "    img_1 = MaxPooling2D((2,2)) (img_1)\n",
    "    img_1 = Dropout(0.1)(img_1)\n",
    "    img_1 = GlobalMaxPooling2D() (img_1)\n",
    "    \n",
    "    \n",
    "    img_2 = Conv2D(128, kernel_size = (3,3), activation=p_activation) ((BatchNormalization(momentum=bn_model))(input_1))\n",
    "    img_2 = MaxPooling2D((2,2)) (img_2)\n",
    "    img_2 = Dropout(0.1)(img_2)\n",
    "    img_2 = GlobalMaxPooling2D() (img_2)\n",
    "    \n",
    "    img_concat =  (Concatenate()([img_1, img_2, BatchNormalization(momentum=bn_model)(input_2)]))\n",
    "    \n",
    "    dense_ayer = Dropout(0.4) (BatchNormalization(momentum=bn_model) ( Dense(512, activation=p_activation)(img_concat) ))\n",
    "    dense_ayer = Dropout(0.4) (BatchNormalization(momentum=bn_model) ( Dense(256, activation=p_activation)(dense_ayer) ))\n",
    "    output = Dense(1, activation=\"sigmoid\")(dense_ayer)\n",
    "    \n",
    "    model = Model([input_1,input_2],  output)\n",
    "    optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def get_model_with_res(img_shape=(75, 75, 2), num_classes=1, f=8, h=512):\n",
    "     \n",
    "    #model\n",
    "    bn_model = 0.99\n",
    "    p_activation = 'elu'\n",
    "    \n",
    "    #\n",
    "    input_img = Input(shape=img_shape, name='img_inputs')\n",
    "    input_img_bn = BatchNormalization(momentum=bn_model)(input_img)\n",
    "    \n",
    "    #\n",
    "    input_meta = Input(shape=[1], name='angle')\n",
    "    input_meta_bn = BatchNormalization(momentum=bn_model)(input_meta)\n",
    "\n",
    "    \n",
    "    #img_1\n",
    "    #img_1:block_1  8\n",
    "    f=32\n",
    "    img_1 = conv_block(input_img_bn, nf=f, k=3, s=1, nb=1,p_act=p_activation)\n",
    "    img_1 = bn_pooling(img_1, k=2, s=2, m=bn_model)\n",
    "    \n",
    "    #img_1:block_2\n",
    "    f=64\n",
    "    img_1 = Dropout(0.2)(img_1)\n",
    "    img_1 = conv_block(img_1, nf=f, k=3, s=1, nb=1,p_act=p_activation)\n",
    "    img_1 = bn_pooling(img_1, k=2, s=2, m=bn_model) \n",
    "    \n",
    "    #img_1:block_2\n",
    "    f=64\n",
    "    img_1 = res_block(img_1, nf=f, k=3, s=1, nb=3)\n",
    "\n",
    "    \n",
    "    #img_1:block_3\n",
    "    f=128\n",
    "\n",
    "    img_1 = conv_block(img_1, nf=f, k=3, s=1, nb=1,p_act=p_activation)\n",
    "    img_1 = bn_pooling(img_1, k=2, s=3, m=bn_model)\n",
    "    img_1 = Dropout(0.2)(img_1)\n",
    "    f=256\n",
    "    img_1 = conv_block(img_1, nf=f, k=3, s=1, nb=1,p_act=p_activation)\n",
    "    img_1 = bn_pooling(img_1, k=2, s=3, m=bn_model)\n",
    "    img_1 = Dropout(0.2)(img_1)\n",
    "    f=512\n",
    "    img_1 = conv_block(img_1, nf=f, k=3, s=1, nb=1,p_act=p_activation)\n",
    "    img_1 = bn_pooling(img_1, k=2, s=3, m=bn_model)\n",
    "    img_1 = Dropout(0.2)(img_1)    \n",
    "    img_1 = GlobalMaxPooling2D()(img_1)\n",
    "    \n",
    "    #full connect\n",
    "    concat = (Concatenate()([img_1, input_meta_bn]))\n",
    "    x = dense_block(img_1, h=h)\n",
    "    x = dense_block(x, h=256)\n",
    "    output = Dense(num_classes, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model([input_img, input_meta], output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_model(img_shape=(75, 75, 2), num_classes=1, f=8, h=512):\n",
    "\n",
    "    \"\"\"\n",
    "    img_shape: dimension for input image\n",
    "    f: filters of first conv blocks and generate filters in the following \n",
    "       blocks acorrdingly \n",
    "    h: units in dense hidden layer\n",
    "    \"\"\" \n",
    "    \n",
    "    #model\n",
    "    bn_model = 0.99\n",
    "    p_activation = 'elu'\n",
    "    \n",
    "    #\n",
    "    input_img = Input(shape=img_shape, name='img_inputs')\n",
    "    input_img_bn = BatchNormalization(momentum=bn_model)(input_img)\n",
    "    \n",
    "    #\n",
    "    input_meta = Input(shape=[1], name='angle')\n",
    "    input_meta_bn = BatchNormalization(momentum=bn_model)(input_meta)\n",
    "\n",
    "    \n",
    "    #img_1\n",
    "    #img_1:block_1  8\n",
    "    img_1 = conv_block(input_img_bn, nf=f, k=3, s=1, nb=6,p_act=p_activation)\n",
    "    img_1 = bn_pooling(img_1, k=2, s=2, m=bn_model)\n",
    "    \n",
    "    #img_1:block_2\n",
    "    f=16\n",
    "    img_1 = Dropout(0.2)(img_1)\n",
    "    img_1 = conv_block(img_1, nf=f, k=3, s=1, nb=6,p_act=p_activation)\n",
    "    img_1 = bn_pooling(img_1, k=2, s=2, m=bn_model) \n",
    "    \n",
    "    #img_1:block_3\n",
    "    f=32\n",
    "    img_1 = Dropout(0.2)(img_1)\n",
    "    img_1 = conv_block(img_1, nf=f, k=3, s=1, nb=6,p_act=p_activation)\n",
    "    img_1 = bn_pooling(img_1, k=2, s=3, m=bn_model)\n",
    "    \n",
    "    #img_1:block_4\n",
    "    f=64\n",
    "    img_1 = Dropout(0.2)(img_1)\n",
    "    img_1 = conv_block(img_1, nf=f, k=3, s=1, nb=6,p_act=p_activation)\n",
    "    img_1 = Dropout(0.2)(img_1)\n",
    "    img_1 = BatchNormalization(momentum=bn_model)(GlobalMaxPooling2D()(img_1))\n",
    "    \n",
    "    #img 2 #32  | tot_layers: 6\n",
    "    f = 8\n",
    "    img_2 = conv_block(input_img_bn, nf=f, k=3, s=1, nb=6,p_act=p_activation)\n",
    "    img_2 = bn_pooling(img_2, k=2, s=3, m=bn_model)\n",
    "    \n",
    "    #img_1:block_2\n",
    "    f=16\n",
    "    img_2 = Dropout(0.2)(img_2)\n",
    "    img_2 = conv_block(img_2, nf=f, k=3, s=1, nb=6,p_act=p_activation)\n",
    "    img_2 = bn_pooling(img_2, k=2, s=2, m=bn_model) \n",
    "    \n",
    "    #img_1:block_3\n",
    "    f=32\n",
    "    img_2 = Dropout(0.2)(img_2)\n",
    "    img_2 = conv_block(img_2, nf=f, k=3, s=1, nb=6,p_act=p_activation)\n",
    "    img_2 = bn_pooling(img_2, k=2, s=3, m=bn_model)\n",
    "    \n",
    "    #img_1:block_4\n",
    "    f=64\n",
    "    img_2 = Dropout(0.2)(img_2)\n",
    "    img_2 = conv_block(img_2, nf=f, k=3, s=1, nb=6,p_act=p_activation)\n",
    "    img_2 = Dropout(0.2)(img_2)\n",
    "    img_2 = BatchNormalization(momentum=bn_model)(GlobalMaxPooling2D()(img_2))\n",
    "    \n",
    "    \n",
    "    #full connect\n",
    "    concat = (Concatenate()([img_1, img_2, input_meta_bn]))\n",
    "    x = dense_block(img_1, h=h)\n",
    "    x = dense_block(x, h=h)\n",
    "    output = Dense(num_classes, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model([input_img, input_meta], output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1203, 75, 75, 3)\n",
      "X_valid: (401, 75, 75, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, X_angle_train, X_angle_valid, y_train, y_valid = train_test_split(rgb_train,\n",
    "                                    X_angle_train, y_train, random_state=420, test_size=0.25)\n",
    "\n",
    "print(\"X_train:\",X_train.shape)\n",
    "print(\"X_valid:\",X_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "X_1 (InputLayer)                (None, 75, 75, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 75, 75, 3)    12          X_1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 71, 71, 16)   1216        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 67, 67, 16)   6416        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 33, 33, 16)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 33, 33, 16)   0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 31, 31, 32)   4640        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 29, 29, 32)   9248        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 14, 14, 32)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 14, 14, 32)   0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 12, 12, 72)   20808       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 10, 10, 72)   46728       conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 5, 5, 72)     0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 5, 5, 72)     0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 75, 75, 3)    12          X_1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 3, 3, 140)    90860       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 73, 73, 140)  3920        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 1, 1, 140)    0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 36, 36, 140)  0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1, 1, 140)    0           max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 36, 36, 140)  0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "angle (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_1 (GlobalM (None, 140)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_2 (GlobalM (None, 140)          0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1)            4           angle[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 281)          0           global_max_pooling2d_1[0][0]     \n",
      "                                                                 global_max_pooling2d_2[0][0]     \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         288768      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 1024)         4096        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 1024)         0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1024)         1049600     dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 1024)         4096        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 1024)         0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            1025        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,531,449\n",
      "Trainable params: 1,527,339\n",
      "Non-trainable params: 4,110\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "#model = get_model(img_shape=(75,75,2))\n",
    "#model = get_model_with_res(img_shape=(75,75,2))\n",
    "#model = get_simple_model()\n",
    "model = get_simple_model_var1()\n",
    "\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "301/301 [==============================] - 28s 94ms/step - loss: 0.5409 - acc: 0.7783 - val_loss: 0.5363 - val_acc: 0.8080\n",
      "Epoch 2/500\n",
      "301/301 [==============================] - 24s 79ms/step - loss: 0.3920 - acc: 0.8271 - val_loss: 0.5621 - val_acc: 0.8130\n",
      "Epoch 3/500\n",
      "301/301 [==============================] - 25s 84ms/step - loss: 0.3410 - acc: 0.8502 - val_loss: 0.2708 - val_acc: 0.8903\n",
      "Epoch 4/500\n",
      "301/301 [==============================] - 27s 89ms/step - loss: 0.3295 - acc: 0.8560 - val_loss: 0.4204 - val_acc: 0.8080\n",
      "Epoch 5/500\n",
      "301/301 [==============================] - 26s 87ms/step - loss: 0.3168 - acc: 0.8636 - val_loss: 0.2470 - val_acc: 0.8953\n",
      "Epoch 6/500\n",
      "301/301 [==============================] - 25s 83ms/step - loss: 0.3045 - acc: 0.8654 - val_loss: 0.3356 - val_acc: 0.8828\n",
      "Epoch 7/500\n",
      "301/301 [==============================] - 25s 83ms/step - loss: 0.2939 - acc: 0.8716 - val_loss: 0.2799 - val_acc: 0.8828\n",
      "Epoch 8/500\n",
      "301/301 [==============================] - 24s 80ms/step - loss: 0.3066 - acc: 0.8634 - val_loss: 0.2371 - val_acc: 0.9102\n",
      "Epoch 9/500\n",
      "301/301 [==============================] - 23s 77ms/step - loss: 0.2819 - acc: 0.8800 - val_loss: 0.2342 - val_acc: 0.9152\n",
      "Epoch 10/500\n",
      "301/301 [==============================] - 23s 77ms/step - loss: 0.2793 - acc: 0.8787 - val_loss: 0.2341 - val_acc: 0.9102\n",
      "Epoch 11/500\n",
      "301/301 [==============================] - 23s 78ms/step - loss: 0.2799 - acc: 0.8771 - val_loss: 0.2282 - val_acc: 0.9177\n",
      "Epoch 12/500\n",
      "301/301 [==============================] - 24s 81ms/step - loss: 0.2749 - acc: 0.8831 - val_loss: 0.3100 - val_acc: 0.8803\n",
      "Epoch 13/500\n",
      "301/301 [==============================] - 25s 82ms/step - loss: 0.2684 - acc: 0.8827 - val_loss: 0.2260 - val_acc: 0.9102\n",
      "Epoch 14/500\n",
      "301/301 [==============================] - 25s 83ms/step - loss: 0.2574 - acc: 0.8895 - val_loss: 0.2308 - val_acc: 0.9202\n",
      "Epoch 15/500\n",
      "301/301 [==============================] - 24s 79ms/step - loss: 0.2629 - acc: 0.8859 - val_loss: 0.2304 - val_acc: 0.9177\n",
      "Epoch 16/500\n",
      "301/301 [==============================] - 24s 78ms/step - loss: 0.2553 - acc: 0.8888 - val_loss: 0.2386 - val_acc: 0.9052\n",
      "Epoch 17/500\n",
      "301/301 [==============================] - 24s 81ms/step - loss: 0.2496 - acc: 0.8899 - val_loss: 0.2273 - val_acc: 0.9227\n",
      "Epoch 18/500\n",
      "301/301 [==============================] - 25s 83ms/step - loss: 0.2534 - acc: 0.8929 - val_loss: 0.2511 - val_acc: 0.9027\n",
      "Epoch 19/500\n",
      "301/301 [==============================] - 25s 82ms/step - loss: 0.2484 - acc: 0.8915 - val_loss: 0.2448 - val_acc: 0.8978\n",
      "Epoch 20/500\n",
      "301/301 [==============================] - 25s 83ms/step - loss: 0.2442 - acc: 0.8966 - val_loss: 0.2285 - val_acc: 0.9177\n",
      "Epoch 21/500\n",
      "301/301 [==============================] - 27s 91ms/step - loss: 0.2424 - acc: 0.8941 - val_loss: 0.2345 - val_acc: 0.9177\n",
      "Epoch 22/500\n",
      "301/301 [==============================] - 25s 83ms/step - loss: 0.2441 - acc: 0.8982 - val_loss: 0.2246 - val_acc: 0.9252\n",
      "Epoch 23/500\n",
      "301/301 [==============================] - 25s 82ms/step - loss: 0.2433 - acc: 0.8974 - val_loss: 0.2355 - val_acc: 0.9102\n",
      "Epoch 24/500\n",
      "301/301 [==============================] - 24s 79ms/step - loss: 0.2419 - acc: 0.8952 - val_loss: 0.2308 - val_acc: 0.9202\n",
      "Epoch 25/500\n",
      "301/301 [==============================] - 24s 79ms/step - loss: 0.2471 - acc: 0.8941 - val_loss: 0.2317 - val_acc: 0.9177\n",
      "Epoch 26/500\n",
      "301/301 [==============================] - 24s 80ms/step - loss: 0.2395 - acc: 0.8938 - val_loss: 0.2378 - val_acc: 0.9127\n",
      "Epoch 27/500\n",
      "301/301 [==============================] - 24s 79ms/step - loss: 0.2333 - acc: 0.8995 - val_loss: 0.2204 - val_acc: 0.9152\n",
      "Epoch 28/500\n",
      "301/301 [==============================] - 24s 81ms/step - loss: 0.2287 - acc: 0.9021 - val_loss: 0.2540 - val_acc: 0.9077\n",
      "Epoch 29/500\n",
      "301/301 [==============================] - 24s 80ms/step - loss: 0.2369 - acc: 0.8983 - val_loss: 0.2440 - val_acc: 0.9127\n",
      "Epoch 30/500\n",
      "301/301 [==============================] - 24s 78ms/step - loss: 0.2339 - acc: 0.9020 - val_loss: 0.2503 - val_acc: 0.9052\n",
      "Epoch 31/500\n",
      "301/301 [==============================] - 24s 79ms/step - loss: 0.2400 - acc: 0.8991 - val_loss: 0.2404 - val_acc: 0.9127\n",
      "Epoch 32/500\n",
      "301/301 [==============================] - 24s 80ms/step - loss: 0.2318 - acc: 0.9003 - val_loss: 0.2258 - val_acc: 0.9127\n",
      "Epoch 33/500\n",
      "301/301 [==============================] - 25s 82ms/step - loss: 0.2337 - acc: 0.8971 - val_loss: 0.2372 - val_acc: 0.9127\n",
      "Epoch 34/500\n",
      "301/301 [==============================] - 24s 79ms/step - loss: 0.2226 - acc: 0.9054 - val_loss: 0.2355 - val_acc: 0.9127\n",
      "Epoch 35/500\n",
      "301/301 [==============================] - 24s 79ms/step - loss: 0.2228 - acc: 0.9060 - val_loss: 0.2284 - val_acc: 0.9127\n",
      "Epoch 36/500\n",
      "301/301 [==============================] - 24s 81ms/step - loss: 0.2231 - acc: 0.9019 - val_loss: 0.2369 - val_acc: 0.9102\n",
      "Epoch 37/500\n",
      "301/301 [==============================] - 23s 78ms/step - loss: 0.2331 - acc: 0.9026 - val_loss: 0.2275 - val_acc: 0.9177\n",
      "Epoch 38/500\n",
      "301/301 [==============================] - 23s 78ms/step - loss: 0.2222 - acc: 0.9054 - val_loss: 0.2323 - val_acc: 0.9052\n",
      "Epoch 39/500\n",
      "301/301 [==============================] - 23s 78ms/step - loss: 0.2253 - acc: 0.9091 - val_loss: 0.2305 - val_acc: 0.9102\n",
      "Epoch 40/500\n",
      "301/301 [==============================] - 24s 81ms/step - loss: 0.2242 - acc: 0.9045 - val_loss: 0.2473 - val_acc: 0.9102\n",
      "Epoch 41/500\n",
      "301/301 [==============================] - 24s 78ms/step - loss: 0.2221 - acc: 0.9052 - val_loss: 0.2305 - val_acc: 0.9177\n",
      "Epoch 42/500\n",
      "301/301 [==============================] - 24s 80ms/step - loss: 0.2226 - acc: 0.9064 - val_loss: 0.2360 - val_acc: 0.9127\n",
      "Epoch 43/500\n",
      "301/301 [==============================] - 25s 83ms/step - loss: 0.2273 - acc: 0.9042 - val_loss: 0.2554 - val_acc: 0.9052\n",
      "Epoch 44/500\n",
      "301/301 [==============================] - 24s 79ms/step - loss: 0.2215 - acc: 0.9084 - val_loss: 0.2494 - val_acc: 0.9077\n",
      "Epoch 45/500\n",
      "301/301 [==============================] - 24s 78ms/step - loss: 0.2123 - acc: 0.9114 - val_loss: 0.2442 - val_acc: 0.9102\n",
      "Epoch 46/500\n",
      "301/301 [==============================] - 23s 77ms/step - loss: 0.2139 - acc: 0.9084 - val_loss: 0.2526 - val_acc: 0.9077\n",
      "Epoch 47/500\n",
      "301/301 [==============================] - 24s 80ms/step - loss: 0.2153 - acc: 0.9106 - val_loss: 0.2531 - val_acc: 0.9052\n",
      "Epoch 48/500\n",
      "301/301 [==============================] - 23s 78ms/step - loss: 0.2176 - acc: 0.9062 - val_loss: 0.2418 - val_acc: 0.9077\n",
      "Epoch 49/500\n",
      "301/301 [==============================] - 24s 79ms/step - loss: 0.2177 - acc: 0.9092 - val_loss: 0.2540 - val_acc: 0.9077\n",
      "Epoch 50/500\n",
      "301/301 [==============================] - 24s 80ms/step - loss: 0.2196 - acc: 0.9059 - val_loss: 0.2541 - val_acc: 0.9027\n",
      "Epoch 51/500\n",
      "301/301 [==============================] - 24s 79ms/step - loss: 0.2168 - acc: 0.9089 - val_loss: 0.2413 - val_acc: 0.9052\n",
      "Epoch 52/500\n",
      "301/301 [==============================] - 24s 78ms/step - loss: 0.2128 - acc: 0.9118 - val_loss: 0.2579 - val_acc: 0.9052\n",
      "Epoch 53/500\n",
      "301/301 [==============================] - 23s 78ms/step - loss: 0.2089 - acc: 0.9117 - val_loss: 0.2526 - val_acc: 0.8978\n",
      "Epoch 54/500\n",
      "301/301 [==============================] - 24s 80ms/step - loss: 0.2219 - acc: 0.9085 - val_loss: 0.2346 - val_acc: 0.8978\n",
      "Epoch 55/500\n",
      "301/301 [==============================] - 25s 82ms/step - loss: 0.2082 - acc: 0.9121 - val_loss: 0.2467 - val_acc: 0.9027\n",
      "Epoch 56/500\n",
      "301/301 [==============================] - 23s 78ms/step - loss: 0.2102 - acc: 0.9138 - val_loss: 0.2490 - val_acc: 0.9052\n",
      "Epoch 57/500\n",
      "301/301 [==============================] - 23s 77ms/step - loss: 0.2119 - acc: 0.9095 - val_loss: 0.2706 - val_acc: 0.9002\n",
      "Epoch 58/500\n",
      "301/301 [==============================] - 24s 78ms/step - loss: 0.2060 - acc: 0.9137 - val_loss: 0.2565 - val_acc: 0.9027\n",
      "Epoch 59/500\n",
      "301/301 [==============================] - 24s 80ms/step - loss: 0.2105 - acc: 0.9074 - val_loss: 0.2537 - val_acc: 0.9077\n",
      "Epoch 60/500\n",
      "301/301 [==============================] - 23s 77ms/step - loss: 0.2072 - acc: 0.9121 - val_loss: 0.2503 - val_acc: 0.9002\n",
      "Epoch 61/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301/301 [==============================] - 23s 77ms/step - loss: 0.2075 - acc: 0.9138 - val_loss: 0.2304 - val_acc: 0.9077\n",
      "Epoch 62/500\n",
      "301/301 [==============================] - 23s 77ms/step - loss: 0.2055 - acc: 0.9127 - val_loss: 0.2619 - val_acc: 0.9052\n",
      "Epoch 63/500\n",
      "301/301 [==============================] - 23s 77ms/step - loss: 0.2019 - acc: 0.9171 - val_loss: 0.2446 - val_acc: 0.9052\n",
      "Epoch 64/500\n",
      "301/301 [==============================] - 23s 77ms/step - loss: 0.2142 - acc: 0.9113 - val_loss: 0.2489 - val_acc: 0.9002\n",
      "Epoch 65/500\n",
      "301/301 [==============================] - 23s 77ms/step - loss: 0.2019 - acc: 0.9137 - val_loss: 0.2551 - val_acc: 0.8978\n",
      "Epoch 66/500\n",
      "301/301 [==============================] - 23s 78ms/step - loss: 0.2006 - acc: 0.9150 - val_loss: 0.2370 - val_acc: 0.9052\n",
      "Epoch 67/500\n",
      "301/301 [==============================] - 23s 77ms/step - loss: 0.2014 - acc: 0.9165 - val_loss: 0.2484 - val_acc: 0.9052\n",
      "Epoch 68/500\n",
      "300/301 [============================>.] - ETA: 0s - loss: 0.2063 - acc: 0.9118\n",
      "Epoch 00068: reducing learning rate to 0.00010000000474974513.\n",
      "301/301 [==============================] - 23s 78ms/step - loss: 0.2069 - acc: 0.9116 - val_loss: 0.2448 - val_acc: 0.9052\n",
      "Epoch 69/500\n",
      "301/301 [==============================] - 23s 77ms/step - loss: 0.2051 - acc: 0.9128 - val_loss: 0.2455 - val_acc: 0.9002\n",
      "Epoch 70/500\n",
      "301/301 [==============================] - 24s 81ms/step - loss: 0.1996 - acc: 0.9161 - val_loss: 0.2471 - val_acc: 0.9027\n",
      "Epoch 71/500\n",
      "301/301 [==============================] - 23s 77ms/step - loss: 0.1968 - acc: 0.9177 - val_loss: 0.2471 - val_acc: 0.9002\n",
      "Epoch 72/500\n",
      "301/301 [==============================] - 23s 77ms/step - loss: 0.1987 - acc: 0.9164 - val_loss: 0.2471 - val_acc: 0.9002\n",
      "Epoch 73/500\n",
      "301/301 [==============================] - 23s 76ms/step - loss: 0.2088 - acc: 0.9129 - val_loss: 0.2463 - val_acc: 0.9027\n",
      "Epoch 74/500\n",
      "301/301 [==============================] - 23s 77ms/step - loss: 0.2033 - acc: 0.9144 - val_loss: 0.2475 - val_acc: 0.9002\n",
      "Epoch 75/500\n",
      "301/301 [==============================] - 23s 76ms/step - loss: 0.2030 - acc: 0.9160 - val_loss: 0.2451 - val_acc: 0.9002\n",
      "Epoch 76/500\n",
      "301/301 [==============================] - 23s 77ms/step - loss: 0.2001 - acc: 0.9191 - val_loss: 0.2484 - val_acc: 0.9002\n",
      "Epoch 77/500\n",
      "301/301 [==============================] - 24s 78ms/step - loss: 0.1991 - acc: 0.9147 - val_loss: 0.2456 - val_acc: 0.9002\n",
      "Epoch 00077: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18b2be70860>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "batch_size = 32\n",
    "#Lets define the image transormations that we want\n",
    "gen = ImageDataGenerator(horizontal_flip=True,\n",
    "                         vertical_flip=True,\n",
    "                         width_shift_range=0.1,\n",
    "                         height_shift_range=0.1,\n",
    "                         zoom_range=0.1,\n",
    "                         rotation_range=90)\n",
    "\n",
    "# Here is the function that merges our two generators\n",
    "def gen_flow_for_two_inputs(X1, X2, y):\n",
    "    genX1 = gen.flow(X1,y,  batch_size=batch_size,seed=666)\n",
    "    genX2 = gen.flow(X1,X2, batch_size=batch_size,seed=666)\n",
    "    while True:\n",
    "            X1i = genX1.next()\n",
    "            X2i = genX2.next()\n",
    "            #Assert arrays are equal - this was for peace of mind, but slows down training\n",
    "            #np.testing.assert_array_equal(X1i[0],X2i[0])\n",
    "            yield [X1i[0], X2i[1]], X1i[1]\n",
    "\n",
    "# Finally create generator\n",
    "gen_flow = gen_flow_for_two_inputs(X_train, X_angle_train, y_train)\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "#call backs\n",
    "weights_file = './model.h5'\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=50, verbose=1, min_delta=1e-4, mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=40, verbose=1, epsilon=1e-4, mode='min')\n",
    "model_chk = ModelCheckpoint(monitor='val_loss', filepath=weights_file, save_best_only=True, \n",
    "                            save_weights_only=True, mode='min')\n",
    "        \n",
    "callbacks = [earlystop, reduce_lr_loss, model_chk, TensorBoard(log_dir='./logs1')]\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.002), \n",
    "              loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#fit the model\n",
    "model.fit_generator(gen_flow, validation_data=([X_valid, X_angle_valid], y_valid), \n",
    "                    steps_per_epoch=np.ceil(8 * float(len(y_train)) / float(batch_size)), \n",
    "                    epochs=500, verbose=1, callbacks=callbacks)\n",
    "#model.fit([X_train, X_angle_train],y_train, validation_data=([X_valid, X_angle_valid], y_valid),  \n",
    " #                   epochs=500, verbose=1, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitted: (1348, 199, 199, 2), (256, 199, 199, 2)\n",
      "splitted: (1348,), (256,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\preprocessing\\image.py:855: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (1348, 199, 199, 2) (2 channels).\n",
      "  ' (' + str(self.x.shape[channels_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      " - 227s - loss: 0.7768 - acc: 0.6080 - val_loss: 0.5501 - val_acc: 0.6562\n",
      "Epoch 2/1000\n",
      " - 223s - loss: 0.6647 - acc: 0.6359 - val_loss: 0.8509 - val_acc: 0.6094\n",
      "Epoch 3/1000\n",
      " - 222s - loss: 0.6292 - acc: 0.6624 - val_loss: 0.6694 - val_acc: 0.7266\n",
      "Epoch 4/1000\n",
      " - 222s - loss: 0.5671 - acc: 0.7160 - val_loss: 0.5135 - val_acc: 0.7695\n",
      "Epoch 5/1000\n",
      " - 223s - loss: 0.5097 - acc: 0.7713 - val_loss: 0.4972 - val_acc: 0.7852\n",
      "Epoch 6/1000\n",
      " - 223s - loss: 0.4549 - acc: 0.7917 - val_loss: 0.4711 - val_acc: 0.8008\n",
      "Epoch 7/1000\n",
      " - 222s - loss: 0.4221 - acc: 0.8051 - val_loss: 0.4220 - val_acc: 0.8125\n",
      "Epoch 8/1000\n",
      " - 222s - loss: 0.4232 - acc: 0.8059 - val_loss: 0.4748 - val_acc: 0.7852\n",
      "Epoch 9/1000\n",
      " - 223s - loss: 0.4080 - acc: 0.8081 - val_loss: 0.3783 - val_acc: 0.8281\n",
      "Epoch 10/1000\n",
      " - 222s - loss: 0.3845 - acc: 0.8217 - val_loss: 0.4678 - val_acc: 0.7930\n",
      "Epoch 11/1000\n",
      " - 222s - loss: 0.3867 - acc: 0.8217 - val_loss: 0.4340 - val_acc: 0.7969\n",
      "Epoch 12/1000\n",
      " - 222s - loss: 0.3755 - acc: 0.8276 - val_loss: 0.4191 - val_acc: 0.7969\n",
      "Epoch 13/1000\n",
      " - 223s - loss: 0.3663 - acc: 0.8284 - val_loss: 0.3671 - val_acc: 0.8320\n",
      "Epoch 14/1000\n",
      " - 222s - loss: 0.3621 - acc: 0.8308 - val_loss: 0.5488 - val_acc: 0.7578\n",
      "Epoch 15/1000\n",
      " - 222s - loss: 0.3624 - acc: 0.8255 - val_loss: 0.6174 - val_acc: 0.7539\n",
      "Epoch 16/1000\n",
      " - 222s - loss: 0.3696 - acc: 0.8270 - val_loss: 0.5350 - val_acc: 0.7852\n",
      "Epoch 17/1000\n",
      " - 222s - loss: 0.3503 - acc: 0.8421 - val_loss: 0.5253 - val_acc: 0.7773\n",
      "Epoch 18/1000\n",
      " - 222s - loss: 0.3463 - acc: 0.8367 - val_loss: 0.5434 - val_acc: 0.7578\n",
      "Epoch 19/1000\n",
      " - 222s - loss: 0.3579 - acc: 0.8379 - val_loss: 0.5331 - val_acc: 0.7812\n",
      "Epoch 20/1000\n",
      " - 222s - loss: 0.3602 - acc: 0.8369 - val_loss: 0.5499 - val_acc: 0.7578\n",
      "Epoch 21/1000\n",
      " - 222s - loss: 0.3499 - acc: 0.8412 - val_loss: 0.5659 - val_acc: 0.7812\n",
      "Epoch 22/1000\n",
      " - 223s - loss: 0.3453 - acc: 0.8397 - val_loss: 0.7128 - val_acc: 0.7227\n",
      "Epoch 23/1000\n",
      " - 222s - loss: 0.3458 - acc: 0.8433 - val_loss: 0.6116 - val_acc: 0.7227\n",
      "Epoch 24/1000\n",
      " - 222s - loss: 0.3458 - acc: 0.8405 - val_loss: 0.5186 - val_acc: 0.7891\n",
      "Epoch 25/1000\n",
      " - 222s - loss: 0.3372 - acc: 0.8428 - val_loss: 0.6632 - val_acc: 0.7227\n",
      "Epoch 26/1000\n",
      " - 223s - loss: 0.3287 - acc: 0.8504 - val_loss: 0.5706 - val_acc: 0.7695\n",
      "Epoch 27/1000\n",
      " - 222s - loss: 0.3316 - acc: 0.8453 - val_loss: 0.4970 - val_acc: 0.7969\n",
      "Epoch 28/1000\n",
      " - 222s - loss: 0.3234 - acc: 0.8505 - val_loss: 0.6877 - val_acc: 0.7188\n",
      "Epoch 29/1000\n",
      " - 222s - loss: 0.3308 - acc: 0.8424 - val_loss: 0.5324 - val_acc: 0.7695\n",
      "Epoch 30/1000\n",
      " - 222s - loss: 0.3364 - acc: 0.8461 - val_loss: 0.6112 - val_acc: 0.7695\n",
      "Epoch 31/1000\n",
      " - 223s - loss: 0.3373 - acc: 0.8473 - val_loss: 0.5197 - val_acc: 0.7773\n",
      "Epoch 32/1000\n",
      " - 222s - loss: 0.3232 - acc: 0.8493 - val_loss: 0.5646 - val_acc: 0.7617\n",
      "Epoch 33/1000\n",
      " - 222s - loss: 0.3215 - acc: 0.8501 - val_loss: 0.4819 - val_acc: 0.7891\n",
      "Epoch 34/1000\n",
      " - 222s - loss: 0.3326 - acc: 0.8487 - val_loss: 0.6395 - val_acc: 0.7070\n",
      "Epoch 35/1000\n",
      " - 223s - loss: 0.3255 - acc: 0.8526 - val_loss: 0.6381 - val_acc: 0.7578\n",
      "Epoch 36/1000\n",
      " - 222s - loss: 0.3294 - acc: 0.8499 - val_loss: 0.7182 - val_acc: 0.7188\n",
      "Epoch 37/1000\n",
      " - 222s - loss: 0.3232 - acc: 0.8547 - val_loss: 0.7364 - val_acc: 0.6992\n",
      "Epoch 38/1000\n",
      " - 222s - loss: 0.3227 - acc: 0.8491 - val_loss: 0.6209 - val_acc: 0.7500\n",
      "Epoch 39/1000\n",
      " - 223s - loss: 0.3238 - acc: 0.8518 - val_loss: 0.5176 - val_acc: 0.7852\n",
      "Epoch 40/1000\n",
      " - 222s - loss: 0.3140 - acc: 0.8567 - val_loss: 0.5834 - val_acc: 0.7773\n",
      "Epoch 41/1000\n",
      " - 222s - loss: 0.3120 - acc: 0.8568 - val_loss: 0.5804 - val_acc: 0.7695\n",
      "Epoch 42/1000\n",
      " - 224s - loss: 0.3205 - acc: 0.8511 - val_loss: 0.6467 - val_acc: 0.7383\n",
      "Epoch 43/1000\n",
      " - 223s - loss: 0.3073 - acc: 0.8547 - val_loss: 0.8042 - val_acc: 0.7188\n",
      "Epoch 44/1000\n",
      " - 223s - loss: 0.3116 - acc: 0.8588 - val_loss: 0.7450 - val_acc: 0.7148\n",
      "Epoch 45/1000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "#train, validataion split\n",
    "test_ratio = 0.159\n",
    "nr_runs = 5\n",
    "split_seed = 25\n",
    "epochs = 1000\n",
    "kf = StratifiedShuffleSplit(n_splits=nr_runs, test_size=test_ratio, train_size=None, random_state=split_seed)\n",
    "\n",
    "train_y = train['is_iceberg'].values\n",
    "split_indices = train_y.copy()\n",
    "\n",
    "train_X = rgb_train\n",
    "train_meta = train['inc_angle'].values\n",
    "\n",
    "#training, evaluation, test and make submission\n",
    "for r, (train_index, valid_index) in enumerate(kf.split(train, split_indices)):\n",
    "    tmp = dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "        \n",
    "    y1, y2 = train_y[train_index], train_y[valid_index]\n",
    "    x1, x2 = train_X[train_index], train_X[valid_index]\n",
    "    xm1, xm2 = train_meta[train_index], train_meta[valid_index]\n",
    "\n",
    "    print('splitted: {0}, {1}'.format(x1.shape, x2.shape), flush=True)\n",
    "    print('splitted: {0}, {1}'.format(y1.shape, y2.shape), flush=True)\n",
    "    ################################\n",
    "    if r > 0:\n",
    "        model.load_weights('model.h5')\n",
    "        \n",
    "    #optim = SGD(lr=0.005, momentum=0.0, decay=0.002, nesterov=True)\n",
    "    optim = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.002)\n",
    "        \n",
    "    model.compile(optimizer=optim, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    #call backs\n",
    "    earlystop = EarlyStopping(monitor='val_loss', patience=100, verbose=1, min_delta=1e-4, mode='min')\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=40, verbose=1, epsilon=1e-4, mode='min')\n",
    "    model_chk = ModelCheckpoint(monitor='val_loss', filepath='model.h5', save_best_only=True, \n",
    "                                save_weights_only=True, mode='min')\n",
    "        \n",
    "    callbacks = [earlystop, reduce_lr_loss, model_chk, TensorBoard(log_dir='./logs_cv')]\n",
    "        ##########\n",
    "       \n",
    "    model.fit_generator(generator=gen_flow_for_two_inputs(x1, xm1, y1),\n",
    "                        steps_per_epoch= np.ceil(12 * float(len(y1)) / float(batch_size)),\n",
    "                        epochs=epochs,\n",
    "                        verbose=2,\n",
    "                        callbacks=callbacks,\n",
    "                        validation_data=([x2, xm2], y2))\n",
    "\n",
    "     \n",
    "\n",
    "    model.load_weights('model.h5')\n",
    "            \n",
    "    p = model.predict([x2, xm2], batch_size=batch_size, verbose=1)\n",
    "    print('\\n\\nEvaluate loss in validation data: {}'.format(log_loss(y2, p)), flush=True)\n",
    "\n",
    "    p = model.predict([x1, xm1], batch_size=batch_size, verbose=1)\n",
    "    print('\\n\\nEvaluate loss in training data: {}'.format(log_loss(y1, p)), flush=True)\n",
    "           \n",
    "    print('\\nPredict...', flush=True)\n",
    "    ids = test['id'].values\n",
    "\n",
    "    #prediction\n",
    "    pred = model.predict([test_X_dup, test_meta], batch_size=batch_size, verbose=1)\n",
    "    pred = np.squeeze(pred, axis=-1)\n",
    "            \n",
    "    file = 'subm_{}_f{:03d}.csv'.format(tmp, nb_filters)\n",
    "    subm = pd.DataFrame({'id': ids, target: pred})\n",
    "    subm.to_csv('./submitions/{}'.format(file), index=False, float_format='%.6f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1363/1363 [==============================] - 1s 957us/step\n",
      "[0.20873701845733716, 0.90755685995539936]\n",
      "241/241 [==============================] - 0s 1ms/step\n",
      "[0.14861328541243224, 0.94605809153362919]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>is_iceberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5941774d</td>\n",
       "      <td>0.383842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4023181e</td>\n",
       "      <td>0.047591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b20200e4</td>\n",
       "      <td>0.035624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e7f018bb</td>\n",
       "      <td>0.986744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4371c8c3</td>\n",
       "      <td>0.069155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  is_iceberg\n",
       "0  5941774d    0.383842\n",
       "1  4023181e    0.047591\n",
       "2  b20200e4    0.035624\n",
       "3  e7f018bb    0.986744\n",
       "4  4371c8c3    0.069155"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on test data\n",
    "from keras.models import load_model\n",
    "model.load_weights('model.h5')\n",
    "\n",
    "print(model.evaluate([X_train,X_angle_train], y_train))\n",
    "print(model.evaluate([X_valid,X_angle_valid], y_valid))\n",
    "\n",
    "test_predictions = model.predict([rgb_test, X_angle_test])\n",
    "\n",
    "tmp = dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "nb_filters = 2\n",
    "file = 'subm_{}_f{:03d}.csv'.format(tmp, nb_filters)\n",
    "\n",
    "# Create .csv\n",
    "pred_df = test[['id']].copy()\n",
    "pred_df['is_iceberg'] = test_predictions\n",
    "pred_df.to_csv('./submitions/{}'.format(file), index=False, float_format='%.6f')\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
